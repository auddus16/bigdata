{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential # 케라스의 Sequential()을 임포트\n",
    "from tensorflow.keras.layers import Dense # 케라스의 Dense()를 임포트\n",
    "from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "# a = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
    "# b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
    "\n",
    "# tensorflow, numpy 랜덤 값을 설정합니다.\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kindNum</th>\n",
       "      <th>neuterYn</th>\n",
       "      <th>sexCd</th>\n",
       "      <th>weight</th>\n",
       "      <th>noticeDays</th>\n",
       "      <th>age2</th>\n",
       "      <th>processState</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.0</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>7.46</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>7.00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.0</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>4.50</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22787</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22788</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22789</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22790</th>\n",
       "      <td>128.0</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22791</th>\n",
       "      <td>114.0</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>3.50</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22777 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kindNum neuterYn sexCd  weight  noticeDays  age2  processState\n",
       "0        128.0        N     F    7.46          10    12             0\n",
       "1        114.0        N     M    7.00          14     1             1\n",
       "2        114.0        U     M    4.50          11     2             0\n",
       "3         67.0        N     M   10.00           8     1             0\n",
       "4        114.0        N     M    6.00           8     4             0\n",
       "...        ...      ...   ...     ...         ...   ...           ...\n",
       "22787    114.0        N     M    1.00          10     0             1\n",
       "22788    114.0        N     M    1.00          10     0             1\n",
       "22789    114.0        N     M    1.00          10     0             1\n",
       "22790    128.0        U     F    6.00          12     3             0\n",
       "22791    114.0        N     M    3.50          10     0             0\n",
       "\n",
       "[22777 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doginfo.csv파일 데이터를 pandas를 이용해 읽어옵니다.\n",
    "dog_data=pd.read_csv(\"doginfo.csv\")\n",
    "dog_train=pd.read_csv(\"trainDoginfo.csv\")\n",
    "kindCd=pd.read_csv(\"kindCd.csv\")\n",
    "\n",
    "dog_data = dog_data.dropna(axis=0)\n",
    "dog_train = dog_train.dropna(axis=0)\n",
    "kindCd_data = kindCd.dropna(axis=0)\n",
    "\n",
    "dog_data\n",
    "# print(dog_train)\n",
    "# print(kindCd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54.  56.  55. 118. 115.  37.  81. 204.  83.  82.  38.  39.  40.  43.\n",
      "  42. 153.  41. 120. 155.  69.  71. 142.  93. 167.  70. 166.  94. 121.\n",
      " 152.  73. 146.  72. 159.  76.  75.  79.  78.  77.  74.  80. 114. 133.\n",
      "  12.  17.  15. 164. 157. 148.  16.  20.  21.  22.  24. 208.  23.  26.\n",
      "  27. 169.  25.  19.  13.  18.  14. 162.  85.  96.  95.   1.  34. 104.\n",
      "  31.  99. 122. 123.  97. 132. 105. 154. 124. 100. 103. 151. 139. 101.\n",
      " 102.  98. 136. 202. 160. 203.   8. 131.   9. 119. 150. 210.  57.  58.\n",
      "  59.   6.   4.   7.   5. 143.  11.  10. 137.  84. 163. 112. 113. 149.\n",
      " 211. 110. 205. 108. 109.  60.  46.  47.  44.  45.  53.  62.  61.  52.\n",
      " 165.  51. 156. 129.  67.  35.  33.  32. 158. 144.  30.  29.  64. 207.\n",
      "  28.   2.  68. 125. 141. 145.  36.  66.  65.  63. 140. 107. 106. 209.\n",
      "  86.  88.  90.  87. 138.  89. 126. 127. 128.  91.   3. 161.  50. 168.\n",
      "  49. 147.  92.  48. 135. 206. 130. 134. 111.]\n",
      "(177,)\n"
     ]
    }
   ],
   "source": [
    "kindCd = np.array(kindCd_data, dtype = np.float64)\n",
    "kindCd_train=np.array(kindCd_data, dtype=np.float64)\n",
    "\n",
    "kindCd = kindCd.reshape(177)\n",
    "kindCd_train=kindCd_train.reshape(177)\n",
    "print(kindCd)\n",
    "print(kindCd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuterYn</th>\n",
       "      <th>sexCd</th>\n",
       "      <th>weight</th>\n",
       "      <th>noticeDays</th>\n",
       "      <th>age2</th>\n",
       "      <th>processState</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>...</th>\n",
       "      <th>202.0</th>\n",
       "      <th>203.0</th>\n",
       "      <th>204.0</th>\n",
       "      <th>205.0</th>\n",
       "      <th>206.0</th>\n",
       "      <th>207.0</th>\n",
       "      <th>208.0</th>\n",
       "      <th>209.0</th>\n",
       "      <th>210.0</th>\n",
       "      <th>211.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>7.46</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>7.00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>4.50</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22787</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22788</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22789</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22790</th>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22791</th>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>3.50</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22777 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      neuterYn sexCd  weight  noticeDays  age2  processState  1.0  2.0  3.0  \\\n",
       "0            N     F    7.46          10    12             0    0    0    0   \n",
       "1            N     M    7.00          14     1             1    0    0    0   \n",
       "2            U     M    4.50          11     2             0    0    0    0   \n",
       "3            N     M   10.00           8     1             0    0    0    0   \n",
       "4            N     M    6.00           8     4             0    0    0    0   \n",
       "...        ...   ...     ...         ...   ...           ...  ...  ...  ...   \n",
       "22787        N     M    1.00          10     0             1    0    0    0   \n",
       "22788        N     M    1.00          10     0             1    0    0    0   \n",
       "22789        N     M    1.00          10     0             1    0    0    0   \n",
       "22790        U     F    6.00          12     3             0    0    0    0   \n",
       "22791        N     M    3.50          10     0             0    0    0    0   \n",
       "\n",
       "       4.0  ...  202.0  203.0  204.0  205.0  206.0  207.0  208.0  209.0  \\\n",
       "0        0  ...      0      0      0      0      0      0      0      0   \n",
       "1        0  ...      0      0      0      0      0      0      0      0   \n",
       "2        0  ...      0      0      0      0      0      0      0      0   \n",
       "3        0  ...      0      0      0      0      0      0      0      0   \n",
       "4        0  ...      0      0      0      0      0      0      0      0   \n",
       "...    ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "22787    0  ...      0      0      0      0      0      0      0      0   \n",
       "22788    0  ...      0      0      0      0      0      0      0      0   \n",
       "22789    0  ...      0      0      0      0      0      0      0      0   \n",
       "22790    0  ...      0      0      0      0      0      0      0      0   \n",
       "22791    0  ...      0      0      0      0      0      0      0      0   \n",
       "\n",
       "       210.0  211.0  \n",
       "0          0      0  \n",
       "1          0      0  \n",
       "2          0      0  \n",
       "3          0      0  \n",
       "4          0      0  \n",
       "...      ...    ...  \n",
       "22787      0      0  \n",
       "22788      0      0  \n",
       "22789      0      0  \n",
       "22790      0      0  \n",
       "22791      0      0  \n",
       "\n",
       "[22777 rows x 183 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kindNum을 원핫 인코딩\n",
    "kindCd = pd.concat((pd.get_dummies(dog_data.kindNum, columns=kindCd), pd.DataFrame(columns=kindCd))).fillna(0)\n",
    "# kindCd\n",
    "\n",
    "# 학습데이터에서 kindNum 열을 삭제한 후, 원핫 인코딩된 kindCd를 붙임\n",
    "dog_data.drop(['kindNum'], axis='columns', inplace=True)\n",
    "dog_data = pd.concat([dog_data, kindCd], axis=1)\n",
    "\n",
    "\n",
    "# kindNum을 원핫 인코딩\n",
    "kindCd_train = pd.concat((pd.get_dummies(dog_train.kindNum, columns=kindCd_train), pd.DataFrame(columns=kindCd_train))).fillna(0)\n",
    "\n",
    "# 테스트데이터에서 kindNum 열을 삭제한 후, 원핫 인코딩된 kindCd를 붙임\n",
    "dog_train.drop(['kindNum'], axis='columns', inplace=True)\n",
    "dog_train = pd.concat([dog_train, kindCd_train], axis=1)\n",
    "dog_data\n",
    "# dog_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_data(data, columns):\n",
    "    for column in columns:\n",
    "        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis = 1)\n",
    "        data = data.drop(column, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       weight  noticeDays  age2  processState  1.0  2.0  3.0  4.0  5.0  6.0  \\\n",
      "0        7.46          10    12             0    0    0    0    0    0    0   \n",
      "1        7.00          14     1             1    0    0    0    0    0    0   \n",
      "2        4.50          11     2             0    0    0    0    0    0    0   \n",
      "3       10.00           8     1             0    0    0    0    0    0    0   \n",
      "4        6.00           8     4             0    0    0    0    0    0    0   \n",
      "...       ...         ...   ...           ...  ...  ...  ...  ...  ...  ...   \n",
      "22787    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22788    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22789    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22790    6.00          12     3             0    0    0    0    0    0    0   \n",
      "22791    3.50          10     0             0    0    0    0    0    0    0   \n",
      "\n",
      "       ...  208.0  209.0  210.0  211.0  neuterYn_N  neuterYn_U  neuterYn_Y  \\\n",
      "0      ...      0      0      0      0           1           0           0   \n",
      "1      ...      0      0      0      0           1           0           0   \n",
      "2      ...      0      0      0      0           0           1           0   \n",
      "3      ...      0      0      0      0           1           0           0   \n",
      "4      ...      0      0      0      0           1           0           0   \n",
      "...    ...    ...    ...    ...    ...         ...         ...         ...   \n",
      "22787  ...      0      0      0      0           1           0           0   \n",
      "22788  ...      0      0      0      0           1           0           0   \n",
      "22789  ...      0      0      0      0           1           0           0   \n",
      "22790  ...      0      0      0      0           0           1           0   \n",
      "22791  ...      0      0      0      0           1           0           0   \n",
      "\n",
      "       sexCd_F  sexCd_M  sexCd_Q  \n",
      "0            1        0        0  \n",
      "1            0        1        0  \n",
      "2            0        1        0  \n",
      "3            0        1        0  \n",
      "4            0        1        0  \n",
      "...        ...      ...      ...  \n",
      "22787        0        1        0  \n",
      "22788        0        1        0  \n",
      "22789        0        1        0  \n",
      "22790        1        0        0  \n",
      "22791        0        1        0  \n",
      "\n",
      "[22777 rows x 187 columns]\n"
     ]
    }
   ],
   "source": [
    "dummy_columns = [\"neuterYn\", \"sexCd\"]\n",
    "data = dummy_data(dog_data, dummy_columns)\n",
    "train_data = dummy_data(dog_train, dummy_columns)\n",
    "data.head()\n",
    "train_data.head()\n",
    "# print(type(dog_data))\n",
    "# print(type(data))\n",
    "\n",
    "print(data)\n",
    "\n",
    "data = np.array(data, dtype = np.float64)\n",
    "train_data = np.array(train_data, dtype = np.float64)\n",
    "\n",
    "# data\n",
    "# print(data.shape)\n",
    "# print(len(train_data))\n",
    "# print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.46 10.   12.   ...  1.    0.    0.  ]\n",
      " [ 7.   14.    1.   ...  0.    1.    0.  ]\n",
      " [ 4.5  11.    2.   ...  0.    1.    0.  ]\n",
      " ...\n",
      " [ 1.   10.    0.   ...  0.    1.    0.  ]\n",
      " [ 6.   12.    3.   ...  1.    0.    0.  ]\n",
      " [ 3.5  10.    0.   ...  0.    1.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22777, 186)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data[:, :3]\n",
    "b = data[:, 4:]\n",
    "\n",
    "# print(a[0,:])\n",
    "# print(b)\n",
    "\n",
    "#numpy 배열에서 데이터 변화요인(kindCd, neuterYn, sexCd, weight, noticeDays, age2)으로 사용할 데이터를 뽑아냅니다.\n",
    "xData = np.concatenate([a, b], axis = 1)\n",
    "\n",
    "print(xData)\n",
    "type(xData)\n",
    "xData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy배열에서 결과(입양여부)로 사용할 데이터를 뽑아냅니다.\n",
    "yData=data[:,[3]]\n",
    "# yData = yData.astype('int32')\n",
    "print(yData)\n",
    "type(yData)\n",
    "yData.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(Dense(1, input_dim=186, activation='sigmoid'))\n",
    "# sgd=optimizers.SGD(lr=0.01)\n",
    "# model.compile(optimizer=sgd ,loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
    "# # 옵티마이저는 경사하강법 sgd를 사용합니다.\n",
    "# # 손실 함수(Loss function)는 binary_crossentropy(이진 크로스 엔트로피)를 사용합니다.\n",
    "# model.fit(xData,yData, batch_size=128, epochs=200, shuffle=False)\n",
    "# # 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 200번 시도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22777 samples, validate on 22777 samples\n",
      "Epoch 1/100\n",
      "22777/22777 [==============================] - 11s 489us/step - loss: 0.6289 - accuracy: 0.6491 - binary_crossentropy: 0.6289 - val_loss: 0.6172 - val_accuracy: 0.6692 - val_binary_crossentropy: 0.6172\n",
      "Epoch 2/100\n",
      "22777/22777 [==============================] - 14s 627us/step - loss: 0.6050 - accuracy: 0.6759 - binary_crossentropy: 0.6050 - val_loss: 0.5990 - val_accuracy: 0.6860 - val_binary_crossentropy: 0.5990\n",
      "Epoch 3/100\n",
      "22777/22777 [==============================] - 14s 595us/step - loss: 0.6008 - accuracy: 0.6773 - binary_crossentropy: 0.6008 - val_loss: 0.6009 - val_accuracy: 0.6852 - val_binary_crossentropy: 0.6009\n",
      "Epoch 4/100\n",
      "22777/22777 [==============================] - 12s 526us/step - loss: 0.5986 - accuracy: 0.6814 - binary_crossentropy: 0.5986 - val_loss: 0.6003 - val_accuracy: 0.6805 - val_binary_crossentropy: 0.6003\n",
      "Epoch 5/100\n",
      "22777/22777 [==============================] - 12s 524us/step - loss: 0.5955 - accuracy: 0.6827 - binary_crossentropy: 0.5955 - val_loss: 0.5901 - val_accuracy: 0.6876 - val_binary_crossentropy: 0.5901\n",
      "Epoch 6/100\n",
      "22777/22777 [==============================] - 14s 617us/step - loss: 0.5952 - accuracy: 0.6825 - binary_crossentropy: 0.5952 - val_loss: 0.5917 - val_accuracy: 0.6885 - val_binary_crossentropy: 0.5917\n",
      "Epoch 7/100\n",
      "22777/22777 [==============================] - 14s 636us/step - loss: 0.5953 - accuracy: 0.6817 - binary_crossentropy: 0.5953 - val_loss: 0.5964 - val_accuracy: 0.6858 - val_binary_crossentropy: 0.5964\n",
      "Epoch 8/100\n",
      "22777/22777 [==============================] - 15s 665us/step - loss: 0.5959 - accuracy: 0.6809 - binary_crossentropy: 0.5959 - val_loss: 0.5862 - val_accuracy: 0.6854 - val_binary_crossentropy: 0.5862\n",
      "Epoch 9/100\n",
      "22777/22777 [==============================] - 16s 711us/step - loss: 0.5957 - accuracy: 0.6836 - binary_crossentropy: 0.5957 - val_loss: 0.5947 - val_accuracy: 0.6848 - val_binary_crossentropy: 0.5947\n",
      "Epoch 10/100\n",
      "22777/22777 [==============================] - 14s 610us/step - loss: 0.5939 - accuracy: 0.6856 - binary_crossentropy: 0.5939 - val_loss: 0.5882 - val_accuracy: 0.6901 - val_binary_crossentropy: 0.5882\n",
      "Epoch 11/100\n",
      "22777/22777 [==============================] - 14s 635us/step - loss: 0.5926 - accuracy: 0.6836 - binary_crossentropy: 0.5926 - val_loss: 0.5869 - val_accuracy: 0.6928 - val_binary_crossentropy: 0.5869\n",
      "Epoch 12/100\n",
      "22777/22777 [==============================] - 14s 595us/step - loss: 0.5927 - accuracy: 0.6849 - binary_crossentropy: 0.5927 - val_loss: 0.5896 - val_accuracy: 0.6862 - val_binary_crossentropy: 0.5896\n",
      "Epoch 13/100\n",
      "22777/22777 [==============================] - 12s 519us/step - loss: 0.5929 - accuracy: 0.6819 - binary_crossentropy: 0.5929 - val_loss: 0.5875 - val_accuracy: 0.6914 - val_binary_crossentropy: 0.5875\n",
      "Epoch 14/100\n",
      "22777/22777 [==============================] - 12s 512us/step - loss: 0.5922 - accuracy: 0.6868 - binary_crossentropy: 0.5922 - val_loss: 0.5867 - val_accuracy: 0.6881 - val_binary_crossentropy: 0.5867\n",
      "Epoch 15/100\n",
      "22777/22777 [==============================] - 14s 598us/step - loss: 0.5902 - accuracy: 0.6836 - binary_crossentropy: 0.5902 - val_loss: 0.5920 - val_accuracy: 0.6874 - val_binary_crossentropy: 0.5920\n",
      "Epoch 16/100\n",
      "22777/22777 [==============================] - 12s 526us/step - loss: 0.5901 - accuracy: 0.6828 - binary_crossentropy: 0.5901 - val_loss: 0.5828 - val_accuracy: 0.6915 - val_binary_crossentropy: 0.5828\n",
      "Epoch 17/100\n",
      "22777/22777 [==============================] - 11s 499us/step - loss: 0.5917 - accuracy: 0.6867 - binary_crossentropy: 0.5917 - val_loss: 0.5837 - val_accuracy: 0.6918 - val_binary_crossentropy: 0.5837\n",
      "Epoch 18/100\n",
      "22777/22777 [==============================] - 13s 578us/step - loss: 0.5906 - accuracy: 0.6856 - binary_crossentropy: 0.5906 - val_loss: 0.5847 - val_accuracy: 0.6899 - val_binary_crossentropy: 0.5847\n",
      "Epoch 19/100\n",
      "22777/22777 [==============================] - 13s 582us/step - loss: 0.5898 - accuracy: 0.6877 - binary_crossentropy: 0.5898 - val_loss: 0.5849 - val_accuracy: 0.6931 - val_binary_crossentropy: 0.5849\n",
      "Epoch 20/100\n",
      "22777/22777 [==============================] - 15s 642us/step - loss: 0.5905 - accuracy: 0.6864 - binary_crossentropy: 0.5905 - val_loss: 0.5818 - val_accuracy: 0.6919 - val_binary_crossentropy: 0.5818\n",
      "Epoch 21/100\n",
      "22777/22777 [==============================] - 13s 553us/step - loss: 0.5898 - accuracy: 0.6871 - binary_crossentropy: 0.5898 - val_loss: 0.5837 - val_accuracy: 0.6903 - val_binary_crossentropy: 0.5837\n",
      "Epoch 22/100\n",
      "22777/22777 [==============================] - 12s 523us/step - loss: 0.5894 - accuracy: 0.6882 - binary_crossentropy: 0.5894 - val_loss: 0.5820 - val_accuracy: 0.6936 - val_binary_crossentropy: 0.5820\n",
      "Epoch 23/100\n",
      "22777/22777 [==============================] - 11s 498us/step - loss: 0.5883 - accuracy: 0.6876 - binary_crossentropy: 0.5883 - val_loss: 0.5829 - val_accuracy: 0.6914 - val_binary_crossentropy: 0.5829\n",
      "Epoch 24/100\n",
      "22777/22777 [==============================] - 11s 464us/step - loss: 0.5884 - accuracy: 0.6905 - binary_crossentropy: 0.5884 - val_loss: 0.5813 - val_accuracy: 0.6919 - val_binary_crossentropy: 0.5813\n",
      "Epoch 25/100\n",
      "22777/22777 [==============================] - 11s 462us/step - loss: 0.5890 - accuracy: 0.6849 - binary_crossentropy: 0.5890 - val_loss: 0.5828 - val_accuracy: 0.6943 - val_binary_crossentropy: 0.5828\n",
      "Epoch 26/100\n",
      "22777/22777 [==============================] - 10s 449us/step - loss: 0.5878 - accuracy: 0.6870 - binary_crossentropy: 0.5878 - val_loss: 0.5809 - val_accuracy: 0.6956 - val_binary_crossentropy: 0.5809\n",
      "Epoch 27/100\n",
      "22777/22777 [==============================] - 14s 607us/step - loss: 0.5871 - accuracy: 0.6896 - binary_crossentropy: 0.5871 - val_loss: 0.5854 - val_accuracy: 0.6908 - val_binary_crossentropy: 0.5854\n",
      "Epoch 28/100\n",
      "22777/22777 [==============================] - 13s 577us/step - loss: 0.5872 - accuracy: 0.6881 - binary_crossentropy: 0.5872 - val_loss: 0.5809 - val_accuracy: 0.6938 - val_binary_crossentropy: 0.5809\n",
      "Epoch 29/100\n",
      "22777/22777 [==============================] - 12s 536us/step - loss: 0.5890 - accuracy: 0.6864 - binary_crossentropy: 0.5890 - val_loss: 0.5840 - val_accuracy: 0.6875 - val_binary_crossentropy: 0.5840\n",
      "Epoch 30/100\n",
      "22777/22777 [==============================] - 11s 500us/step - loss: 0.5860 - accuracy: 0.6916 - binary_crossentropy: 0.5860 - val_loss: 0.5828 - val_accuracy: 0.6929 - val_binary_crossentropy: 0.5828\n",
      "Epoch 31/100\n",
      "22777/22777 [==============================] - 12s 531us/step - loss: 0.5869 - accuracy: 0.6903 - binary_crossentropy: 0.5869 - val_loss: 0.5808 - val_accuracy: 0.6921 - val_binary_crossentropy: 0.5808\n",
      "Epoch 32/100\n",
      "22777/22777 [==============================] - 14s 619us/step - loss: 0.5896 - accuracy: 0.6877 - binary_crossentropy: 0.5896 - val_loss: 0.5816 - val_accuracy: 0.6932 - val_binary_crossentropy: 0.5816\n",
      "Epoch 33/100\n",
      "22777/22777 [==============================] - 14s 594us/step - loss: 0.5883 - accuracy: 0.6876 - binary_crossentropy: 0.5883 - val_loss: 0.5809 - val_accuracy: 0.6945 - val_binary_crossentropy: 0.5809\n",
      "Epoch 34/100\n",
      "22777/22777 [==============================] - 12s 525us/step - loss: 0.5887 - accuracy: 0.6882 - binary_crossentropy: 0.5887 - val_loss: 0.5837 - val_accuracy: 0.6849 - val_binary_crossentropy: 0.5837\n",
      "Epoch 35/100\n",
      "22777/22777 [==============================] - 11s 485us/step - loss: 0.5875 - accuracy: 0.6885 - binary_crossentropy: 0.5875 - val_loss: 0.5806 - val_accuracy: 0.6961 - val_binary_crossentropy: 0.5806\n",
      "Epoch 36/100\n",
      "22777/22777 [==============================] - 13s 555us/step - loss: 0.5876 - accuracy: 0.6890 - binary_crossentropy: 0.5876 - val_loss: 0.5809 - val_accuracy: 0.6943 - val_binary_crossentropy: 0.5809\n",
      "Epoch 37/100\n",
      "22777/22777 [==============================] - 13s 583us/step - loss: 0.5863 - accuracy: 0.6899 - binary_crossentropy: 0.5863 - val_loss: 0.5840 - val_accuracy: 0.6926 - val_binary_crossentropy: 0.5840\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22777/22777 [==============================] - 12s 547us/step - loss: 0.5864 - accuracy: 0.6875 - binary_crossentropy: 0.5864 - val_loss: 0.5846 - val_accuracy: 0.6924 - val_binary_crossentropy: 0.5846\n",
      "Epoch 39/100\n",
      "22777/22777 [==============================] - 11s 478us/step - loss: 0.5874 - accuracy: 0.6858 - binary_crossentropy: 0.5874 - val_loss: 0.5780 - val_accuracy: 0.6958 - val_binary_crossentropy: 0.5780\n",
      "Epoch 40/100\n",
      "22777/22777 [==============================] - 12s 543us/step - loss: 0.5860 - accuracy: 0.6899 - binary_crossentropy: 0.5860 - val_loss: 0.5797 - val_accuracy: 0.6946 - val_binary_crossentropy: 0.5797\n",
      "Epoch 41/100\n",
      "22777/22777 [==============================] - 13s 576us/step - loss: 0.5859 - accuracy: 0.6895 - binary_crossentropy: 0.5859 - val_loss: 0.5809 - val_accuracy: 0.6916 - val_binary_crossentropy: 0.5809\n",
      "Epoch 42/100\n",
      "22777/22777 [==============================] - 10s 434us/step - loss: 0.5852 - accuracy: 0.6925 - binary_crossentropy: 0.5852 - val_loss: 0.5806 - val_accuracy: 0.6932 - val_binary_crossentropy: 0.5806\n",
      "Epoch 43/100\n",
      "22777/22777 [==============================] - 11s 493us/step - loss: 0.5872 - accuracy: 0.6900 - binary_crossentropy: 0.5872 - val_loss: 0.5783 - val_accuracy: 0.6947 - val_binary_crossentropy: 0.5783\n",
      "Epoch 44/100\n",
      "22777/22777 [==============================] - 13s 559us/step - loss: 0.5871 - accuracy: 0.6885 - binary_crossentropy: 0.5871 - val_loss: 0.5802 - val_accuracy: 0.6920 - val_binary_crossentropy: 0.5802\n",
      "Epoch 45/100\n",
      "22777/22777 [==============================] - 14s 636us/step - loss: 0.5874 - accuracy: 0.6890 - binary_crossentropy: 0.5874 - val_loss: 0.5793 - val_accuracy: 0.6959 - val_binary_crossentropy: 0.5793\n",
      "Epoch 46/100\n",
      "22777/22777 [==============================] - 13s 551us/step - loss: 0.5880 - accuracy: 0.6909 - binary_crossentropy: 0.5880 - val_loss: 0.5818 - val_accuracy: 0.6900 - val_binary_crossentropy: 0.5818\n",
      "Epoch 47/100\n",
      "22777/22777 [==============================] - 11s 488us/step - loss: 0.5851 - accuracy: 0.6929 - binary_crossentropy: 0.5851 - val_loss: 0.5822 - val_accuracy: 0.6943 - val_binary_crossentropy: 0.5822\n",
      "Epoch 48/100\n",
      "22777/22777 [==============================] - 11s 480us/step - loss: 0.5846 - accuracy: 0.6913 - binary_crossentropy: 0.5846 - val_loss: 0.5800 - val_accuracy: 0.6957 - val_binary_crossentropy: 0.5800\n",
      "Epoch 49/100\n",
      "22777/22777 [==============================] - 12s 533us/step - loss: 0.5864 - accuracy: 0.6902 - binary_crossentropy: 0.5864 - val_loss: 0.5778 - val_accuracy: 0.6958 - val_binary_crossentropy: 0.5778\n",
      "Epoch 50/100\n",
      "22777/22777 [==============================] - 9s 399us/step - loss: 0.5861 - accuracy: 0.6891 - binary_crossentropy: 0.5861 - val_loss: 0.5791 - val_accuracy: 0.6947 - val_binary_crossentropy: 0.5791\n",
      "Epoch 51/100\n",
      "22777/22777 [==============================] - 11s 480us/step - loss: 0.5858 - accuracy: 0.6891 - binary_crossentropy: 0.5858 - val_loss: 0.5784 - val_accuracy: 0.6932 - val_binary_crossentropy: 0.5784\n",
      "Epoch 52/100\n",
      "22777/22777 [==============================] - 14s 594us/step - loss: 0.5863 - accuracy: 0.6909 - binary_crossentropy: 0.5863 - val_loss: 0.5771 - val_accuracy: 0.6974 - val_binary_crossentropy: 0.5771\n",
      "Epoch 53/100\n",
      "22777/22777 [==============================] - 12s 522us/step - loss: 0.5855 - accuracy: 0.6879 - binary_crossentropy: 0.5855 - val_loss: 0.5772 - val_accuracy: 0.6948 - val_binary_crossentropy: 0.5772\n",
      "Epoch 54/100\n",
      "22777/22777 [==============================] - 11s 499us/step - loss: 0.5850 - accuracy: 0.6914 - binary_crossentropy: 0.5850 - val_loss: 0.5775 - val_accuracy: 0.6962 - val_binary_crossentropy: 0.5775\n",
      "Epoch 55/100\n",
      "22777/22777 [==============================] - 11s 487us/step - loss: 0.5858 - accuracy: 0.6900 - binary_crossentropy: 0.5858 - val_loss: 0.5791 - val_accuracy: 0.6968 - val_binary_crossentropy: 0.5791\n",
      "Epoch 56/100\n",
      "22777/22777 [==============================] - 14s 603us/step - loss: 0.5861 - accuracy: 0.6914 - binary_crossentropy: 0.5861 - val_loss: 0.5773 - val_accuracy: 0.6974 - val_binary_crossentropy: 0.5773\n",
      "Epoch 57/100\n",
      "22777/22777 [==============================] - 13s 574us/step - loss: 0.5848 - accuracy: 0.6895 - binary_crossentropy: 0.5848 - val_loss: 0.5795 - val_accuracy: 0.6938 - val_binary_crossentropy: 0.5795\n",
      "Epoch 58/100\n",
      "22777/22777 [==============================] - 12s 534us/step - loss: 0.5836 - accuracy: 0.6904 - binary_crossentropy: 0.5836 - val_loss: 0.5786 - val_accuracy: 0.6928 - val_binary_crossentropy: 0.5786\n",
      "Epoch 59/100\n",
      "22777/22777 [==============================] - 11s 501us/step - loss: 0.5840 - accuracy: 0.6917 - binary_crossentropy: 0.5840 - val_loss: 0.5784 - val_accuracy: 0.6976 - val_binary_crossentropy: 0.5784\n",
      "Epoch 60/100\n",
      "22777/22777 [==============================] - 12s 537us/step - loss: 0.5833 - accuracy: 0.6918 - binary_crossentropy: 0.5833 - val_loss: 0.5771 - val_accuracy: 0.6967 - val_binary_crossentropy: 0.5771\n",
      "Epoch 61/100\n",
      "22777/22777 [==============================] - 14s 605us/step - loss: 0.5826 - accuracy: 0.6924 - binary_crossentropy: 0.5826 - val_loss: 0.5767 - val_accuracy: 0.6936 - val_binary_crossentropy: 0.5767\n",
      "Epoch 62/100\n",
      "22777/22777 [==============================] - 11s 488us/step - loss: 0.5833 - accuracy: 0.6905 - binary_crossentropy: 0.5833 - val_loss: 0.5807 - val_accuracy: 0.6948 - val_binary_crossentropy: 0.5807\n",
      "Epoch 63/100\n",
      "22777/22777 [==============================] - 12s 510us/step - loss: 0.5842 - accuracy: 0.6889 - binary_crossentropy: 0.5842 - val_loss: 0.5779 - val_accuracy: 0.6987 - val_binary_crossentropy: 0.5779\n",
      "Epoch 64/100\n",
      "22777/22777 [==============================] - 12s 522us/step - loss: 0.5829 - accuracy: 0.6921 - binary_crossentropy: 0.5829 - val_loss: 0.5767 - val_accuracy: 0.6960 - val_binary_crossentropy: 0.5767\n",
      "Epoch 65/100\n",
      "22777/22777 [==============================] - 14s 621us/step - loss: 0.5837 - accuracy: 0.6940 - binary_crossentropy: 0.5837 - val_loss: 0.5783 - val_accuracy: 0.6921 - val_binary_crossentropy: 0.5783\n",
      "Epoch 66/100\n",
      "22777/22777 [==============================] - 13s 570us/step - loss: 0.5834 - accuracy: 0.6906 - binary_crossentropy: 0.5834 - val_loss: 0.5752 - val_accuracy: 0.6979 - val_binary_crossentropy: 0.5752\n",
      "Epoch 67/100\n",
      "22777/22777 [==============================] - 11s 499us/step - loss: 0.5838 - accuracy: 0.6904 - binary_crossentropy: 0.5838 - val_loss: 0.5759 - val_accuracy: 0.6963 - val_binary_crossentropy: 0.5759\n",
      "Epoch 68/100\n",
      "22777/22777 [==============================] - 11s 472us/step - loss: 0.5839 - accuracy: 0.6916 - binary_crossentropy: 0.5839 - val_loss: 0.5767 - val_accuracy: 0.6968 - val_binary_crossentropy: 0.5767\n",
      "Epoch 69/100\n",
      "22777/22777 [==============================] - 10s 451us/step - loss: 0.5850 - accuracy: 0.6916 - binary_crossentropy: 0.5850 - val_loss: 0.5767 - val_accuracy: 0.6982 - val_binary_crossentropy: 0.5767\n",
      "Epoch 70/100\n",
      "22777/22777 [==============================] - 12s 534us/step - loss: 0.5833 - accuracy: 0.6899 - binary_crossentropy: 0.5833 - val_loss: 0.5772 - val_accuracy: 0.6970 - val_binary_crossentropy: 0.5772\n",
      "Epoch 71/100\n",
      "22777/22777 [==============================] - 10s 451us/step - loss: 0.5832 - accuracy: 0.6948 - binary_crossentropy: 0.5832 - val_loss: 0.5758 - val_accuracy: 0.6965 - val_binary_crossentropy: 0.5758\n",
      "Epoch 72/100\n",
      "22777/22777 [==============================] - 10s 421us/step - loss: 0.5827 - accuracy: 0.6894 - binary_crossentropy: 0.5827 - val_loss: 0.5760 - val_accuracy: 0.6964 - val_binary_crossentropy: 0.5760\n",
      "Epoch 73/100\n",
      "22777/22777 [==============================] - 12s 510us/step - loss: 0.5834 - accuracy: 0.6901 - binary_crossentropy: 0.5834 - val_loss: 0.5793 - val_accuracy: 0.6904 - val_binary_crossentropy: 0.5793\n",
      "Epoch 74/100\n",
      "22777/22777 [==============================] - 11s 494us/step - loss: 0.5844 - accuracy: 0.6915 - binary_crossentropy: 0.5844 - val_loss: 0.5753 - val_accuracy: 0.6986 - val_binary_crossentropy: 0.5753\n",
      "Epoch 75/100\n",
      "22777/22777 [==============================] - 13s 566us/step - loss: 0.5831 - accuracy: 0.6940 - binary_crossentropy: 0.5831 - val_loss: 0.5759 - val_accuracy: 0.6981 - val_binary_crossentropy: 0.5759\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22777/22777 [==============================] - 10s 433us/step - loss: 0.5830 - accuracy: 0.6918 - binary_crossentropy: 0.5830 - val_loss: 0.5761 - val_accuracy: 0.6969 - val_binary_crossentropy: 0.5761\n",
      "Epoch 77/100\n",
      "22777/22777 [==============================] - 11s 499us/step - loss: 0.5834 - accuracy: 0.6906 - binary_crossentropy: 0.5834 - val_loss: 0.5753 - val_accuracy: 0.6979 - val_binary_crossentropy: 0.5753\n",
      "Epoch 78/100\n",
      "22777/22777 [==============================] - 11s 491us/step - loss: 0.5842 - accuracy: 0.6902 - binary_crossentropy: 0.5842 - val_loss: 0.5764 - val_accuracy: 0.6977 - val_binary_crossentropy: 0.5764\n",
      "Epoch 79/100\n",
      "22777/22777 [==============================] - 13s 571us/step - loss: 0.5849 - accuracy: 0.6924 - binary_crossentropy: 0.5849 - val_loss: 0.5761 - val_accuracy: 0.6958 - val_binary_crossentropy: 0.5761\n",
      "Epoch 80/100\n",
      "22777/22777 [==============================] - 12s 538us/step - loss: 0.5826 - accuracy: 0.6900 - binary_crossentropy: 0.5826 - val_loss: 0.5784 - val_accuracy: 0.6969 - val_binary_crossentropy: 0.5784\n",
      "Epoch 81/100\n",
      "22777/22777 [==============================] - 12s 511us/step - loss: 0.5833 - accuracy: 0.6898 - binary_crossentropy: 0.5833 - val_loss: 0.5744 - val_accuracy: 0.6987 - val_binary_crossentropy: 0.5744\n",
      "Epoch 82/100\n",
      "22777/22777 [==============================] - 12s 527us/step - loss: 0.5819 - accuracy: 0.6950 - binary_crossentropy: 0.5819 - val_loss: 0.5748 - val_accuracy: 0.6983 - val_binary_crossentropy: 0.5748\n",
      "Epoch 83/100\n",
      "22777/22777 [==============================] - 14s 619us/step - loss: 0.5814 - accuracy: 0.6927 - binary_crossentropy: 0.5814 - val_loss: 0.5747 - val_accuracy: 0.6980 - val_binary_crossentropy: 0.5747\n",
      "Epoch 84/100\n",
      "22777/22777 [==============================] - 13s 565us/step - loss: 0.5822 - accuracy: 0.6923 - binary_crossentropy: 0.5822 - val_loss: 0.5792 - val_accuracy: 0.6929 - val_binary_crossentropy: 0.5792\n",
      "Epoch 85/100\n",
      "22777/22777 [==============================] - 12s 522us/step - loss: 0.5840 - accuracy: 0.6914 - binary_crossentropy: 0.5840 - val_loss: 0.5743 - val_accuracy: 0.6991 - val_binary_crossentropy: 0.5743\n",
      "Epoch 86/100\n",
      "22777/22777 [==============================] - 11s 488us/step - loss: 0.5835 - accuracy: 0.6934 - binary_crossentropy: 0.5835 - val_loss: 0.5780 - val_accuracy: 0.6958 - val_binary_crossentropy: 0.5780\n",
      "Epoch 87/100\n",
      "22777/22777 [==============================] - 12s 505us/step - loss: 0.5817 - accuracy: 0.6920 - binary_crossentropy: 0.5817 - val_loss: 0.5744 - val_accuracy: 0.6982 - val_binary_crossentropy: 0.5744\n",
      "Epoch 88/100\n",
      "22777/22777 [==============================] - 12s 543us/step - loss: 0.5822 - accuracy: 0.6903 - binary_crossentropy: 0.5822 - val_loss: 0.5749 - val_accuracy: 0.6982 - val_binary_crossentropy: 0.5749\n",
      "Epoch 89/100\n",
      "22777/22777 [==============================] - 11s 487us/step - loss: 0.5818 - accuracy: 0.6921 - binary_crossentropy: 0.5818 - val_loss: 0.5734 - val_accuracy: 0.6985 - val_binary_crossentropy: 0.5734\n",
      "Epoch 90/100\n",
      "22777/22777 [==============================] - 11s 467us/step - loss: 0.5818 - accuracy: 0.6942 - binary_crossentropy: 0.5818 - val_loss: 0.5745 - val_accuracy: 0.6975 - val_binary_crossentropy: 0.5745\n",
      "Epoch 91/100\n",
      "22777/22777 [==============================] - 14s 599us/step - loss: 0.5821 - accuracy: 0.6938 - binary_crossentropy: 0.5821 - val_loss: 0.5755 - val_accuracy: 0.6976 - val_binary_crossentropy: 0.5755\n",
      "Epoch 92/100\n",
      "22777/22777 [==============================] - 11s 492us/step - loss: 0.5830 - accuracy: 0.6912 - binary_crossentropy: 0.5830 - val_loss: 0.5756 - val_accuracy: 0.6964 - val_binary_crossentropy: 0.5756\n",
      "Epoch 93/100\n",
      "22777/22777 [==============================] - 12s 531us/step - loss: 0.5814 - accuracy: 0.6932 - binary_crossentropy: 0.5814 - val_loss: 0.5752 - val_accuracy: 0.6971 - val_binary_crossentropy: 0.5752\n",
      "Epoch 94/100\n",
      "22777/22777 [==============================] - 10s 447us/step - loss: 0.5819 - accuracy: 0.6927 - binary_crossentropy: 0.5819 - val_loss: 0.5749 - val_accuracy: 0.6983 - val_binary_crossentropy: 0.5749\n",
      "Epoch 95/100\n",
      "22777/22777 [==============================] - 11s 499us/step - loss: 0.5804 - accuracy: 0.6938 - binary_crossentropy: 0.5804 - val_loss: 0.5754 - val_accuracy: 0.6980 - val_binary_crossentropy: 0.5754\n",
      "Epoch 96/100\n",
      "22777/22777 [==============================] - 12s 544us/step - loss: 0.5812 - accuracy: 0.6932 - binary_crossentropy: 0.5812 - val_loss: 0.5749 - val_accuracy: 0.7004 - val_binary_crossentropy: 0.5749\n",
      "Epoch 97/100\n",
      "22777/22777 [==============================] - 12s 505us/step - loss: 0.5831 - accuracy: 0.6924 - binary_crossentropy: 0.5831 - val_loss: 0.5758 - val_accuracy: 0.6968 - val_binary_crossentropy: 0.5758\n",
      "Epoch 98/100\n",
      "22777/22777 [==============================] - 12s 511us/step - loss: 0.5823 - accuracy: 0.6924 - binary_crossentropy: 0.5823 - val_loss: 0.5766 - val_accuracy: 0.6935 - val_binary_crossentropy: 0.5766\n",
      "Epoch 99/100\n",
      "22777/22777 [==============================] - 13s 556us/step - loss: 0.5797 - accuracy: 0.6930 - binary_crossentropy: 0.5797 - val_loss: 0.5739 - val_accuracy: 0.6980 - val_binary_crossentropy: 0.5739\n",
      "Epoch 100/100\n",
      "22777/22777 [==============================] - 12s 547us/step - loss: 0.5814 - accuracy: 0.6913 - binary_crossentropy: 0.5814 - val_loss: 0.5759 - val_accuracy: 0.6926 - val_binary_crossentropy: 0.5759\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 16)                2992      \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,945\n",
      "Trainable params: 4,785\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# l2_model = keras.models.Sequential([\n",
    "#     keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                        activation=tf.nn.relu, input_shape=(186,)),\n",
    "#     keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                        activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "# ])\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "\n",
    "#     model = Sequential([\n",
    "#     Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                        activation=tf.nn.relu, input_shape=(186,)),\n",
    "#     Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                        activation=tf.nn.relu),\n",
    "#     Dense(1, activation=tf.nn.sigmoid)\n",
    "# ])\n",
    "\n",
    "    model.add(Dense(16, input_shape = (186, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Dense(50, input_shape = (186, )))\n",
    "#     model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "#     model.add(Activation('sigmoid'))    \n",
    "#     model.add(Dense(50))\n",
    "#     model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "#     model.add(Activation('sigmoid'))    \n",
    "#     model.add(Dense(50))\n",
    "#     model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "#     model.add(Activation('sigmoid'))    \n",
    "#     model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))   \n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation('softmax'))\n",
    "\n",
    "#     sgd=optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=Adadelta(rho=0.95),\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy', 'binary_crossentropy'])\n",
    "    return model\n",
    "\n",
    "# mlp_model(l2_model)\n",
    "\n",
    "# sgd=optimizers.SGD(lr=0.01)\n",
    "\n",
    "# l2_model.compile(optimizer='sgd',\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "l2_model = mlp_model()\n",
    "\n",
    "l2_model_history = l2_model.fit(xData, yData,\n",
    "                                epochs=100,\n",
    "                                shuffle = True,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(xData, yData),\n",
    "                                verbose=1)\n",
    "\n",
    "l2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입양 확률 :  94.2203%\n",
      "입양 확률 :  36.7390%\n",
      "입양 확률 :  35.8996%\n",
      "입양 확률 :  78.0932%\n",
      "입양 확률 :  21.0893%\n",
      "입양 확률 :  21.0893%\n",
      "입양 확률 :  91.3862%\n",
      "입양 확률 :  78.4416%\n",
      "입양 확률 :  89.5666%\n",
      "입양 확률 :  84.8468%\n",
      "입양 확률 :  72.6395%\n",
      "입양 확률 :  85.3167%\n",
      "입양 확률 :  55.5937%\n",
      "입양 확률 :  49.5257%\n",
      "입양 확률 :  51.3644%\n",
      "입양 확률 :  28.9706%\n",
      "입양 확률 :  44.0449%\n",
      "입양 확률 :  37.9210%\n",
      "입양 확률 :  83.8701%\n",
      "입양 확률 :  23.1563%\n",
      "입양 확률 :  64.8517%\n",
      "입양 확률 :  64.8517%\n",
      "입양 확률 :  53.8228%\n",
      "입양 확률 :  46.9142%\n",
      "입양 확률 :  71.4867%\n",
      "입양 확률 :  58.0059%\n",
      "입양 확률 :  54.1604%\n",
      "입양 확률 :  57.8769%\n",
      "입양 확률 :  41.1052%\n",
      "입양 확률 :  54.8027%\n",
      "입양 확률 :  56.1087%\n",
      "입양 확률 :  41.1052%\n",
      "입양 확률 :  40.0840%\n",
      "입양 확률 :  37.2096%\n",
      "입양 확률 :  90.8167%\n",
      "입양 확률 :  88.6520%\n",
      "입양 확률 :  19.2490%\n",
      "입양 확률 :  69.2019%\n",
      "입양 확률 :  76.6373%\n",
      "입양 확률 :  28.4616%\n",
      "입양 확률 :  26.2082%\n",
      "입양 확률 :  37.8162%\n",
      "입양 확률 :  49.3927%\n",
      "입양 확률 :  31.0500%\n",
      "입양 확률 :  32.5425%\n",
      "입양 확률 :  45.5628%\n",
      "입양 확률 :  49.3927%\n",
      "입양 확률 :  51.3507%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  45.9654%\n",
      "입양 확률 :  29.5362%\n",
      "입양 확률 :  29.1457%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  52.2923%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  51.3507%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  32.1351%\n",
      "입양 확률 :  67.2334%\n",
      "입양 확률 :  56.3661%\n",
      "입양 확률 :  57.1026%\n",
      "입양 확률 :  55.8354%\n",
      "입양 확률 :  37.6229%\n",
      "입양 확률 :  93.1751%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  78.7943%\n",
      "입양 확률 :  18.4336%\n",
      "입양 확률 :  80.2123%\n",
      "입양 확률 :  72.8484%\n",
      "입양 확률 :  51.0164%\n",
      "입양 확률 :  53.1521%\n",
      "입양 확률 :  77.7039%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  37.9588%\n",
      "입양 확률 :  42.8758%\n",
      "입양 확률 :  51.4375%\n",
      "입양 확률 :  34.8071%\n",
      "입양 확률 :  90.5109%\n",
      "입양 확률 :  23.0991%\n",
      "입양 확률 :  74.9863%\n",
      "입양 확률 :  48.8126%\n",
      "입양 확률 :  52.8710%\n",
      "입양 확률 :  48.1297%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.9207%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  23.6678%\n",
      "입양 확률 :  75.7269%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  57.5152%\n",
      "입양 확률 :  50.9764%\n",
      "입양 확률 :  90.9335%\n",
      "입양 확률 :  34.4362%\n",
      "입양 확률 :  80.4421%\n",
      "입양 확률 :  86.2377%\n",
      "입양 확률 :  24.7418%\n",
      "입양 확률 :  62.9177%\n",
      "입양 확률 :  85.2348%\n",
      "입양 확률 :  28.6270%\n",
      "입양 확률 :  84.8854%\n",
      "입양 확률 :  65.3681%\n",
      "입양 확률 :  29.6700%\n",
      "입양 확률 :  42.5086%\n",
      "입양 확률 :  71.8598%\n",
      "입양 확률 :  37.7539%\n",
      "입양 확률 :  32.4765%\n",
      "입양 확률 :  38.1661%\n",
      "입양 확률 :  86.8071%\n",
      "입양 확률 :  79.7014%\n",
      "입양 확률 :  22.1511%\n",
      "입양 확률 :  22.1511%\n",
      "입양 확률 :  81.6765%\n",
      "입양 확률 :  26.6789%\n",
      "입양 확률 :  65.5374%\n",
      "입양 확률 :  48.4780%\n",
      "입양 확률 :  63.2604%\n",
      "입양 확률 :  89.2275%\n",
      "입양 확률 :  30.8939%\n",
      "입양 확률 :  37.8441%\n",
      "입양 확률 :  38.0527%\n",
      "입양 확률 :  29.3965%\n",
      "입양 확률 :  76.1217%\n",
      "입양 확률 :  83.7344%\n",
      "입양 확률 :  82.3084%\n",
      "입양 확률 :  35.1551%\n",
      "입양 확률 :  80.5109%\n",
      "입양 확률 :  94.7565%\n",
      "입양 확률 :  78.0636%\n",
      "입양 확률 :  59.0956%\n",
      "입양 확률 :  61.4141%\n",
      "입양 확률 :  25.9249%\n",
      "입양 확률 :  94.6280%\n",
      "입양 확률 :  94.1429%\n",
      "입양 확률 :  59.0332%\n",
      "입양 확률 :  90.5268%\n",
      "입양 확률 :  89.4795%\n",
      "입양 확률 :  90.4354%\n",
      "입양 확률 :  28.0512%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  31.5406%\n",
      "입양 확률 :  35.0414%\n",
      "입양 확률 :  29.4471%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  86.8184%\n",
      "입양 확률 :  44.3399%\n",
      "입양 확률 :  70.0254%\n",
      "입양 확률 :  89.8535%\n",
      "입양 확률 :  43.6976%\n",
      "입양 확률 :  49.5257%\n",
      "입양 확률 :  24.8949%\n",
      "입양 확률 :  31.8996%\n",
      "입양 확률 :  28.3564%\n",
      "입양 확률 :  69.9115%\n",
      "입양 확률 :  87.0251%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  66.9685%\n",
      "입양 확률 :  78.0211%\n",
      "입양 확률 :  39.7200%\n",
      "입양 확률 :  29.8014%\n",
      "입양 확률 :  87.8142%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  92.1527%\n",
      "입양 확률 :  82.1423%\n",
      "입양 확률 :  49.8603%\n",
      "입양 확률 :  91.5583%\n",
      "입양 확률 :  73.2581%\n",
      "입양 확률 :  89.3106%\n",
      "입양 확률 :  86.8845%\n",
      "입양 확률 :  36.3459%\n",
      "입양 확률 :  74.8865%\n",
      "입양 확률 :  26.7738%\n",
      "입양 확률 :  76.2915%\n",
      "입양 확률 :  55.9331%\n",
      "입양 확률 :  91.4863%\n",
      "입양 확률 :  29.6403%\n",
      "입양 확률 :  77.2823%\n",
      "입양 확률 :  84.6162%\n",
      "입양 확률 :  44.8503%\n",
      "입양 확률 :  80.0782%\n",
      "입양 확률 :  32.7578%\n",
      "입양 확률 :  39.8271%\n",
      "입양 확률 :  37.7214%\n",
      "입양 확률 :  41.7302%\n",
      "입양 확률 :  90.8608%\n",
      "입양 확률 :  31.8540%\n",
      "입양 확률 :  43.6976%\n",
      "입양 확률 :  39.6527%\n",
      "입양 확률 :  40.5991%\n",
      "입양 확률 :  86.8982%\n",
      "입양 확률 :  46.6477%\n",
      "입양 확률 :  34.0550%\n",
      "입양 확률 :  91.7288%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  82.7919%\n",
      "입양 확률 :  39.7626%\n",
      "입양 확률 :  75.2077%\n",
      "입양 확률 :  38.4672%\n",
      "입양 확률 :  80.2841%\n",
      "입양 확률 :  74.9565%\n",
      "입양 확률 :  80.4142%\n",
      "입양 확률 :  84.9896%\n",
      "입양 확률 :  72.0752%\n",
      "입양 확률 :  27.7959%\n",
      "입양 확률 :  52.4399%\n",
      "입양 확률 :  54.0417%\n",
      "입양 확률 :  40.3163%\n",
      "입양 확률 :  55.1982%\n",
      "입양 확률 :  95.1264%\n",
      "입양 확률 :  73.9199%\n",
      "입양 확률 :  26.0297%\n",
      "입양 확률 :  49.9497%\n",
      "입양 확률 :  40.4894%\n",
      "입양 확률 :  71.9049%\n",
      "입양 확률 :  80.2524%\n",
      "입양 확률 :  53.8228%\n",
      "입양 확률 :  30.5333%\n",
      "입양 확률 :  36.2937%\n",
      "입양 확률 :  44.1794%\n",
      "입양 확률 :  28.2019%\n",
      "입양 확률 :  71.3230%\n",
      "입양 확률 :  71.3230%\n",
      "입양 확률 :  62.0971%\n",
      "입양 확률 :  67.3646%\n",
      "입양 확률 :  91.4151%\n",
      "입양 확률 :  78.5369%\n",
      "입양 확률 :  33.3205%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  60.2206%\n",
      "입양 확률 :  24.6817%\n",
      "입양 확률 :  93.9052%\n",
      "입양 확률 :  67.0049%\n",
      "입양 확률 :  67.2633%\n",
      "입양 확률 :  48.2329%\n",
      "입양 확률 :  87.2070%\n",
      "입양 확률 :  37.5005%\n",
      "입양 확률 :  55.5821%\n",
      "입양 확률 :  86.9302%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  29.1846%\n",
      "입양 확률 :  83.3939%\n",
      "입양 확률 :  79.4073%\n",
      "입양 확률 :  30.9433%\n",
      "입양 확률 :  32.0811%\n",
      "입양 확률 :  30.2279%\n",
      "입양 확률 :  56.1816%\n",
      "입양 확률 :  29.3114%\n",
      "입양 확률 :  88.4804%\n",
      "입양 확률 :  37.4983%\n",
      "입양 확률 :  85.4370%\n",
      "입양 확률 :  29.4495%\n",
      "입양 확률 :  34.0754%\n",
      "입양 확률 :  91.5203%\n",
      "입양 확률 :  40.3446%\n",
      "입양 확률 :  40.7096%\n",
      "입양 확률 :  82.0429%\n",
      "입양 확률 :  43.7205%\n",
      "입양 확률 :  94.1259%\n",
      "입양 확률 :  93.7749%\n",
      "입양 확률 :  32.7578%\n",
      "입양 확률 :  89.7685%\n",
      "입양 확률 :  57.5454%\n",
      "입양 확률 :  27.6363%\n",
      "입양 확률 :  33.2161%\n",
      "입양 확률 :  23.2291%\n",
      "입양 확률 :  42.7267%\n",
      "입양 확률 :  85.2137%\n",
      "입양 확률 :  40.6669%\n",
      "입양 확률 :  84.5175%\n",
      "입양 확률 :  54.4749%\n",
      "입양 확률 :  48.5200%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  28.2052%\n",
      "입양 확률 :  76.6274%\n",
      "입양 확률 :  90.2871%\n",
      "입양 확률 :  63.5538%\n",
      "입양 확률 :  25.3717%\n",
      "입양 확률 :  46.1768%\n",
      "입양 확률 :  45.3451%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  36.7390%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  88.9029%\n",
      "입양 확률 :  33.5215%\n",
      "입양 확률 :  83.5877%\n",
      "입양 확률 :  84.4071%\n",
      "입양 확률 :  44.9242%\n",
      "입양 확률 :  93.1353%\n",
      "입양 확률 :  93.0980%\n",
      "입양 확률 :  71.1721%\n",
      "입양 확률 :  33.2620%\n",
      "입양 확률 :  88.0267%\n",
      "입양 확률 :  27.7433%\n",
      "입양 확률 :  82.5484%\n",
      "입양 확률 :  23.8201%\n",
      "입양 확률 :  70.6739%\n",
      "입양 확률 :  27.7433%\n",
      "입양 확률 :  90.6982%\n",
      "입양 확률 :  27.3487%\n",
      "입양 확률 :  34.1240%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  47.1705%\n",
      "입양 확률 :  51.7130%\n",
      "입양 확률 :  33.3122%\n",
      "입양 확률 :  29.6297%\n",
      "입양 확률 :  76.7626%\n",
      "입양 확률 :  53.2499%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  76.6945%\n",
      "입양 확률 :  90.6940%\n",
      "입양 확률 :  88.4009%\n",
      "입양 확률 :  75.0052%\n",
      "입양 확률 :  23.1924%\n",
      "입양 확률 :  37.5005%\n",
      "입양 확률 :  30.2065%\n",
      "입양 확률 :  54.0701%\n",
      "입양 확률 :  68.7554%\n",
      "입양 확률 :  84.6845%\n",
      "입양 확률 :  36.9821%\n",
      "입양 확률 :  78.4158%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  40.4196%\n",
      "입양 확률 :  86.4652%\n",
      "입양 확률 :  82.5119%\n",
      "입양 확률 :  24.6226%\n",
      "입양 확률 :  30.2970%\n",
      "입양 확률 :  69.4716%\n",
      "입양 확률 :  93.0375%\n",
      "입양 확률 :  33.6043%\n",
      "입양 확률 :  90.0557%\n",
      "입양 확률 :  50.7865%\n",
      "입양 확률 :  33.3205%\n",
      "입양 확률 :  32.4314%\n",
      "입양 확률 :  82.5378%\n",
      "입양 확률 :  29.6516%\n",
      "입양 확률 :  74.6187%\n",
      "입양 확률 :  33.0861%\n",
      "입양 확률 :  76.5208%\n",
      "입양 확률 :  39.1582%\n",
      "입양 확률 :  39.1582%\n",
      "입양 확률 :  27.7513%\n",
      "입양 확률 :  85.2101%\n",
      "입양 확률 :  29.1949%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  34.1780%\n",
      "입양 확률 :  37.3953%\n",
      "입양 확률 :  39.9117%\n",
      "입양 확률 :  30.6148%\n",
      "입양 확률 :  27.3846%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  84.6056%\n",
      "입양 확률 :  29.5503%\n",
      "입양 확률 :  80.6437%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  29.0381%\n",
      "입양 확률 :  85.7431%\n",
      "입양 확률 :  29.5508%\n",
      "입양 확률 :  29.5007%\n",
      "입양 확률 :  29.5007%\n",
      "입양 확률 :  36.3402%\n",
      "입양 확률 :  22.9240%\n",
      "입양 확률 :  68.0388%\n",
      "입양 확률 :  45.0437%\n",
      "입양 확률 :  51.8110%\n",
      "입양 확률 :  91.0538%\n",
      "입양 확률 :  92.6616%\n",
      "입양 확률 :  27.0242%\n",
      "입양 확률 :  36.8133%\n",
      "입양 확률 :  73.4615%\n",
      "입양 확률 :  64.4615%\n",
      "입양 확률 :  92.7757%\n",
      "입양 확률 :  82.4978%\n",
      "입양 확률 :  56.0847%\n",
      "입양 확률 :  57.7592%\n",
      "입양 확률 :  73.3543%\n",
      "입양 확률 :  43.9328%\n",
      "입양 확률 :  84.2009%\n",
      "입양 확률 :  43.5355%\n",
      "입양 확률 :  22.8924%\n",
      "입양 확률 :  72.6577%\n",
      "입양 확률 :  29.1657%\n",
      "입양 확률 :  88.6260%\n",
      "입양 확률 :  42.2912%\n",
      "입양 확률 :  36.0405%\n",
      "입양 확률 :  94.3621%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  29.5317%\n",
      "입양 확률 :  75.3754%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  58.0729%\n",
      "입양 확률 :  31.4567%\n",
      "입양 확률 :  25.0710%\n",
      "입양 확률 :  28.8452%\n",
      "입양 확률 :  94.4262%\n",
      "입양 확률 :  38.4951%\n",
      "입양 확률 :  36.7675%\n",
      "입양 확률 :  20.2064%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  77.4118%\n",
      "입양 확률 :  44.3877%\n",
      "입양 확률 :  91.2746%\n",
      "입양 확률 :  58.3606%\n",
      "입양 확률 :  58.3606%\n",
      "입양 확률 :  57.1406%\n",
      "입양 확률 :  94.4116%\n",
      "입양 확률 :  66.8996%\n",
      "입양 확률 :  36.2556%\n",
      "입양 확률 :  89.6569%\n",
      "입양 확률 :  77.3612%\n",
      "입양 확률 :  79.5870%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  91.3021%\n",
      "입양 확률 :  21.5133%\n",
      "입양 확률 :  21.5133%\n",
      "입양 확률 :  53.4980%\n",
      "입양 확률 :  39.2845%\n",
      "입양 확률 :  42.2721%\n",
      "입양 확률 :  32.6121%\n",
      "입양 확률 :  33.0232%\n",
      "입양 확률 :  91.6300%\n",
      "입양 확률 :  87.2243%\n",
      "입양 확률 :  86.9037%\n",
      "입양 확률 :  20.0483%\n",
      "입양 확률 :  50.5222%\n",
      "입양 확률 :  44.1166%\n",
      "입양 확률 :  57.5966%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  92.5027%\n",
      "입양 확률 :  33.8111%\n",
      "입양 확률 :  35.8180%\n",
      "입양 확률 :  73.8789%\n",
      "입양 확률 :  19.6655%\n",
      "입양 확률 :  20.9785%\n",
      "입양 확률 :  48.2864%\n",
      "입양 확률 :  55.5369%\n",
      "입양 확률 :  82.0286%\n",
      "입양 확률 :  28.0213%\n",
      "입양 확률 :  25.0031%\n",
      "입양 확률 :  71.9830%\n",
      "입양 확률 :  69.3335%\n",
      "입양 확률 :  94.2947%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  79.8950%\n",
      "입양 확률 :  79.5124%\n",
      "입양 확률 :  65.0101%\n",
      "입양 확률 :  92.5092%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입양 확률 :  28.6237%\n",
      "입양 확률 :  93.3353%\n",
      "입양 확률 :  32.6441%\n",
      "입양 확률 :  37.7122%\n",
      "입양 확률 :  67.2223%\n",
      "입양 확률 :  23.0380%\n",
      "입양 확률 :  70.3892%\n",
      "입양 확률 :  77.7126%\n",
      "입양 확률 :  55.1501%\n",
      "입양 확률 :  57.7917%\n",
      "입양 확률 :  67.0752%\n",
      "입양 확률 :  32.6870%\n",
      "입양 확률 :  88.6794%\n",
      "입양 확률 :  50.1714%\n",
      "입양 확률 :  34.4334%\n",
      "입양 확률 :  30.2486%\n",
      "입양 확률 :  57.1593%\n",
      "입양 확률 :  53.8719%\n",
      "입양 확률 :  71.0483%\n",
      "입양 확률 :  66.8481%\n",
      "입양 확률 :  83.2895%\n",
      "입양 확률 :  59.9851%\n",
      "입양 확률 :  87.3223%\n",
      "입양 확률 :  94.8906%\n",
      "입양 확률 :  53.2306%\n",
      "입양 확률 :  41.5467%\n",
      "입양 확률 :  41.7502%\n",
      "입양 확률 :  26.5065%\n",
      "입양 확률 :  29.7671%\n",
      "입양 확률 :  96.1752%\n",
      "입양 확률 :  44.4502%\n",
      "입양 확률 :  92.8657%\n",
      "입양 확률 :  39.4909%\n",
      "입양 확률 :  83.0424%\n",
      "입양 확률 :  91.3181%\n",
      "입양 확률 :  86.7357%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  23.3083%\n",
      "입양 확률 :  24.4934%\n",
      "입양 확률 :  90.5356%\n",
      "입양 확률 :  65.2899%\n",
      "입양 확률 :  32.4555%\n",
      "입양 확률 :  20.8052%\n",
      "입양 확률 :  31.4452%\n",
      "입양 확률 :  50.1362%\n",
      "입양 확률 :  52.8710%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.4489%\n",
      "입양 확률 :  85.6347%\n",
      "입양 확률 :  89.7685%\n",
      "입양 확률 :  30.2853%\n",
      "입양 확률 :  57.8625%\n",
      "입양 확률 :  33.0917%\n",
      "입양 확률 :  52.3523%\n",
      "입양 확률 :  90.5433%\n",
      "입양 확률 :  94.5191%\n",
      "입양 확률 :  36.3402%\n",
      "입양 확률 :  26.6885%\n",
      "입양 확률 :  58.0436%\n",
      "입양 확률 :  34.7650%\n",
      "입양 확률 :  37.8441%\n",
      "입양 확률 :  36.5673%\n",
      "입양 확률 :  52.7714%\n",
      "입양 확률 :  43.0492%\n",
      "입양 확률 :  90.9383%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  92.8120%\n",
      "입양 확률 :  80.4843%\n",
      "입양 확률 :  56.6195%\n",
      "입양 확률 :  75.4732%\n",
      "입양 확률 :  73.3082%\n",
      "입양 확률 :  69.7706%\n",
      "입양 확률 :  46.3540%\n",
      "입양 확률 :  27.0691%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  32.0362%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  51.0287%\n",
      "입양 확률 :  56.4868%\n",
      "입양 확률 :  55.1727%\n",
      "입양 확률 :  27.1687%\n",
      "입양 확률 :  79.5345%\n",
      "입양 확률 :  78.0818%\n",
      "입양 확률 :  26.6011%\n",
      "입양 확률 :  27.5421%\n",
      "입양 확률 :  27.5421%\n",
      "입양 확률 :  26.6011%\n",
      "입양 확률 :  35.2598%\n",
      "입양 확률 :  28.5249%\n",
      "입양 확률 :  24.5133%\n",
      "입양 확률 :  82.6250%\n",
      "입양 확률 :  69.2903%\n",
      "입양 확률 :  26.1384%\n",
      "입양 확률 :  48.7082%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  32.1908%\n",
      "입양 확률 :  70.5507%\n",
      "입양 확률 :  35.4440%\n",
      "입양 확률 :  92.4061%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  30.7316%\n",
      "입양 확률 :  40.7673%\n",
      "입양 확률 :  32.9232%\n",
      "입양 확률 :  29.0579%\n",
      "입양 확률 :  32.0362%\n",
      "입양 확률 :  35.4440%\n",
      "입양 확률 :  86.6981%\n",
      "입양 확률 :  78.8287%\n",
      "입양 확률 :  35.2400%\n",
      "입양 확률 :  23.0991%\n",
      "입양 확률 :  81.9916%\n",
      "입양 확률 :  39.6089%\n",
      "입양 확률 :  74.2966%\n",
      "입양 확률 :  35.8483%\n",
      "입양 확률 :  77.1089%\n",
      "입양 확률 :  38.0417%\n",
      "입양 확률 :  77.1674%\n",
      "입양 확률 :  75.7726%\n",
      "입양 확률 :  87.7377%\n",
      "입양 확률 :  80.3187%\n",
      "입양 확률 :  64.9833%\n",
      "입양 확률 :  59.9470%\n",
      "입양 확률 :  59.2415%\n",
      "입양 확률 :  83.3474%\n",
      "입양 확률 :  29.4303%\n",
      "입양 확률 :  90.9939%\n",
      "입양 확률 :  81.1342%\n",
      "입양 확률 :  61.0305%\n",
      "입양 확률 :  82.8998%\n",
      "입양 확률 :  76.6702%\n",
      "입양 확률 :  34.4896%\n",
      "입양 확률 :  43.6037%\n",
      "입양 확률 :  32.9014%\n",
      "입양 확률 :  23.4589%\n",
      "입양 확률 :  39.5481%\n",
      "입양 확률 :  49.2593%\n",
      "입양 확률 :  51.5592%\n",
      "입양 확률 :  51.5592%\n",
      "입양 확률 :  88.9153%\n",
      "입양 확률 :  90.1937%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    new_x=train_data[i, :].reshape(1,186)\n",
    "    print('입양 확률 : %8.4f%%' % (l2_model.predict(new_x)*100))\n",
    "l2_model.save('l2_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
