{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential # 케라스의 Sequential()을 임포트\n",
    "from tensorflow.keras.layers import Dense # 케라스의 Dense()를 임포트\n",
    "from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#doginfo.csv파일 데이터를 pandas를 이용해 읽어옵니다.\n",
    "dog_data=pd.read_csv(\"doginfo.csv\")\n",
    "dog_train=pd.read_csv(\"testDog.csv\")\n",
    "kindCd=pd.read_csv(\"kindCd.csv\")\n",
    "\n",
    "dog_data = dog_data.dropna(axis=0)\n",
    "dog_train = dog_train.dropna(axis=0)\n",
    "kindCd_data = kindCd.dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54.  56.  55. 118. 115.  37.  81. 204.  83.  82.  38.  39.  40.  43.\n",
      "  42. 153.  41. 120. 155.  69.  71. 142.  93. 167.  70. 166.  94. 121.\n",
      " 152.  73. 146.  72. 159.  76.  75.  79.  78.  77.  74.  80. 114. 133.\n",
      "  12.  17.  15. 164. 157. 148.  16.  20.  21.  22.  24. 208.  23.  26.\n",
      "  27. 169.  25.  19.  13.  18.  14. 162.  85.  96.  95.   1.  34. 104.\n",
      "  31.  99. 122. 123.  97. 132. 105. 154. 124. 100. 103. 151. 139. 101.\n",
      " 102.  98. 136. 202. 160. 203.   8. 131.   9. 119. 150. 210.  57.  58.\n",
      "  59.   6.   4.   7.   5. 143.  11.  10. 137.  84. 163. 112. 113. 149.\n",
      " 211. 110. 205. 108. 109.  60.  46.  47.  44.  45.  53.  62.  61.  52.\n",
      " 165.  51. 156. 129.  67.  35.  33.  32. 158. 144.  30.  29.  64. 207.\n",
      "  28.   2.  68. 125. 141. 145.  36.  66.  65.  63. 140. 107. 106. 209.\n",
      "  86.  88.  90.  87. 138.  89. 126. 127. 128.  91.   3. 161.  50. 168.\n",
      "  49. 147.  92.  48. 135. 206. 130. 134. 111.]\n",
      "(177,)\n"
     ]
    }
   ],
   "source": [
    "kindCd = np.array(kindCd_data, dtype = np.float64)\n",
    "kindCd_train=np.array(kindCd_data, dtype=np.float64)\n",
    "\n",
    "kindCd = kindCd.reshape(177)\n",
    "kindCd_train=kindCd_train.reshape(177)\n",
    "print(kindCd)\n",
    "print(kindCd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kindNum을 원핫 인코딩\n",
    "kindCd = pd.concat((pd.get_dummies(dog_data.kindNum, columns=kindCd), pd.DataFrame(columns=kindCd))).fillna(0)\n",
    "\n",
    "\n",
    "# 학습데이터에서 kindNum 열을 삭제한 후, 원핫 인코딩된 kindCd를 붙임\n",
    "dog_data.drop(['kindNum'], axis='columns', inplace=True)\n",
    "dog_data = pd.concat([dog_data, kindCd], axis=1)\n",
    "\n",
    "\n",
    "# kindNum을 원핫 인코딩\n",
    "kindCd_train = pd.concat((pd.get_dummies(dog_train.kindNum, columns=kindCd_train), pd.DataFrame(columns=kindCd_train))).fillna(0)\n",
    "\n",
    "# 테스트데이터에서 kindNum 열을 삭제한 후, 원핫 인코딩된 kindCd를 붙임\n",
    "dog_train.drop(['kindNum'], axis='columns', inplace=True)\n",
    "dog_train = pd.concat([dog_train, kindCd_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_data(data, columns):\n",
    "    for column in columns:\n",
    "        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis = 1)\n",
    "        data = data.drop(column, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       weight  noticeDays  age2  processState  1.0  2.0  3.0  4.0  5.0  6.0  \\\n",
      "0        7.46          10    12             0    0    0    0    0    0    0   \n",
      "1        7.00          14     1             1    0    0    0    0    0    0   \n",
      "2        4.50          11     2             0    0    0    0    0    0    0   \n",
      "3       10.00           8     1             0    0    0    0    0    0    0   \n",
      "4        6.00           8     4             0    0    0    0    0    0    0   \n",
      "...       ...         ...   ...           ...  ...  ...  ...  ...  ...  ...   \n",
      "22787    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22788    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22789    1.00          10     0             1    0    0    0    0    0    0   \n",
      "22790    6.00          12     3             0    0    0    0    0    0    0   \n",
      "22791    3.50          10     0             0    0    0    0    0    0    0   \n",
      "\n",
      "       ...  208.0  209.0  210.0  211.0  neuterYn_N  neuterYn_U  neuterYn_Y  \\\n",
      "0      ...      0      0      0      0           1           0           0   \n",
      "1      ...      0      0      0      0           1           0           0   \n",
      "2      ...      0      0      0      0           0           1           0   \n",
      "3      ...      0      0      0      0           1           0           0   \n",
      "4      ...      0      0      0      0           1           0           0   \n",
      "...    ...    ...    ...    ...    ...         ...         ...         ...   \n",
      "22787  ...      0      0      0      0           1           0           0   \n",
      "22788  ...      0      0      0      0           1           0           0   \n",
      "22789  ...      0      0      0      0           1           0           0   \n",
      "22790  ...      0      0      0      0           0           1           0   \n",
      "22791  ...      0      0      0      0           1           0           0   \n",
      "\n",
      "       sexCd_F  sexCd_M  sexCd_Q  \n",
      "0            1        0        0  \n",
      "1            0        1        0  \n",
      "2            0        1        0  \n",
      "3            0        1        0  \n",
      "4            0        1        0  \n",
      "...        ...      ...      ...  \n",
      "22787        0        1        0  \n",
      "22788        0        1        0  \n",
      "22789        0        1        0  \n",
      "22790        1        0        0  \n",
      "22791        0        1        0  \n",
      "\n",
      "[22777 rows x 187 columns]\n"
     ]
    }
   ],
   "source": [
    "dummy_columns = [\"neuterYn\", \"sexCd\"]\n",
    "data = dummy_data(dog_data, dummy_columns)\n",
    "train_data = dummy_data(dog_train, dummy_columns)\n",
    "\n",
    "print(data)\n",
    "\n",
    "data = np.array(data, dtype = np.float64)\n",
    "train_data = np.array(train_data, dtype = np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 186)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data[:, :3]\n",
    "b = data[:, 4:]\n",
    "\n",
    "#numpy 배열에서 데이터 변화요인(kindCd, neuterYn, sexCd, weight, noticeDays, age2)으로 사용할 데이터를 뽑아냅니다.\n",
    "xData = np.concatenate([a, b], axis = 1)\n",
    "\n",
    "a=train_data[:, :3]\n",
    "b=train_data[:, 4:]\n",
    "testX=np.concatenate([a, b], axis = 1)\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy배열에서 결과(입양여부)로 사용할 데이터를 뽑아냅니다.\n",
    "yData=data[:,[3]]\n",
    "testY=train_data[:,[3]]\n",
    "\n",
    "print(yData)\n",
    "type(yData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22777 samples, validate on 22777 samples\n",
      "Epoch 1/100\n",
      "22777/22777 [==============================] - 2s 76us/sample - loss: 1.0845 - acc: 0.5942 - val_loss: 1.0281 - val_acc: 0.5902\n",
      "Epoch 2/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.9676 - acc: 0.6305 - val_loss: 0.9181 - val_acc: 0.6466\n",
      "Epoch 3/100\n",
      "22777/22777 [==============================] - 2s 97us/sample - loss: 0.8785 - acc: 0.6408 - val_loss: 0.8455 - val_acc: 0.6392\n",
      "Epoch 4/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.8156 - acc: 0.6487 - val_loss: 0.7866 - val_acc: 0.6655\n",
      "Epoch 5/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.7685 - acc: 0.6568 - val_loss: 0.7461 - val_acc: 0.6579\n",
      "Epoch 6/100\n",
      "22777/22777 [==============================] - 2s 85us/sample - loss: 0.7358 - acc: 0.6544 - val_loss: 0.7158 - val_acc: 0.6642\n",
      "Epoch 7/100\n",
      "22777/22777 [==============================] - 2s 95us/sample - loss: 0.7099 - acc: 0.6570 - val_loss: 0.7286 - val_acc: 0.6356\n",
      "Epoch 8/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6908 - acc: 0.6600 - val_loss: 0.6769 - val_acc: 0.6737\n",
      "Epoch 9/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6765 - acc: 0.6626 - val_loss: 0.7063 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "22777/22777 [==============================] - 2s 101us/sample - loss: 0.6662 - acc: 0.6608 - val_loss: 0.6741 - val_acc: 0.6505\n",
      "Epoch 11/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6583 - acc: 0.6627 - val_loss: 0.6495 - val_acc: 0.6659\n",
      "Epoch 12/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6519 - acc: 0.6655 - val_loss: 0.6508 - val_acc: 0.6567\n",
      "Epoch 13/100\n",
      "22777/22777 [==============================] - 2s 74us/sample - loss: 0.6476 - acc: 0.6651 - val_loss: 0.6364 - val_acc: 0.6777\n",
      "Epoch 14/100\n",
      "22777/22777 [==============================] - 2s 97us/sample - loss: 0.6439 - acc: 0.6641 - val_loss: 0.6343 - val_acc: 0.6748\n",
      "Epoch 15/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6419 - acc: 0.6644 - val_loss: 0.6342 - val_acc: 0.6696\n",
      "Epoch 16/100\n",
      "22777/22777 [==============================] - 2s 78us/sample - loss: 0.6388 - acc: 0.6640 - val_loss: 0.6333 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "22777/22777 [==============================] - 2s 100us/sample - loss: 0.6367 - acc: 0.6666 - val_loss: 0.6313 - val_acc: 0.6649\n",
      "Epoch 18/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6358 - acc: 0.6646 - val_loss: 0.6296 - val_acc: 0.6741\n",
      "Epoch 19/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6359 - acc: 0.6627 - val_loss: 0.6285 - val_acc: 0.6676\n",
      "Epoch 20/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6338 - acc: 0.6655 - val_loss: 0.6259 - val_acc: 0.6718\n",
      "Epoch 21/100\n",
      "22777/22777 [==============================] - 2s 102us/sample - loss: 0.6338 - acc: 0.6618 - val_loss: 0.6266 - val_acc: 0.6695\n",
      "Epoch 22/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6334 - acc: 0.6646 - val_loss: 0.6300 - val_acc: 0.6557\n",
      "Epoch 23/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6326 - acc: 0.6673 - val_loss: 0.6315 - val_acc: 0.6642\n",
      "Epoch 24/100\n",
      "22777/22777 [==============================] - 2s 75us/sample - loss: 0.6325 - acc: 0.6634 - val_loss: 0.6738 - val_acc: 0.6375\n",
      "Epoch 25/100\n",
      "22777/22777 [==============================] - 3s 131us/sample - loss: 0.6314 - acc: 0.6657 - val_loss: 0.6255 - val_acc: 0.6703\n",
      "Epoch 26/100\n",
      "22777/22777 [==============================] - 1s 66us/sample - loss: 0.6312 - acc: 0.6676 - val_loss: 0.6415 - val_acc: 0.6614\n",
      "Epoch 27/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6314 - acc: 0.6631 - val_loss: 0.6222 - val_acc: 0.6801\n",
      "Epoch 28/100\n",
      "22777/22777 [==============================] - 2s 90us/sample - loss: 0.6308 - acc: 0.6648 - val_loss: 0.6267 - val_acc: 0.6696\n",
      "Epoch 29/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6323 - acc: 0.6636 - val_loss: 0.6287 - val_acc: 0.6638\n",
      "Epoch 30/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6318 - acc: 0.6629 - val_loss: 0.6215 - val_acc: 0.6792\n",
      "Epoch 31/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6313 - acc: 0.6645 - val_loss: 0.6240 - val_acc: 0.6722\n",
      "Epoch 32/100\n",
      "22777/22777 [==============================] - 2s 109us/sample - loss: 0.6309 - acc: 0.6641 - val_loss: 0.6233 - val_acc: 0.6720\n",
      "Epoch 33/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6302 - acc: 0.6665 - val_loss: 0.6238 - val_acc: 0.6771\n",
      "Epoch 34/100\n",
      "22777/22777 [==============================] - 2s 81us/sample - loss: 0.6309 - acc: 0.6667 - val_loss: 0.6220 - val_acc: 0.6750\n",
      "Epoch 35/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6289 - acc: 0.6656 - val_loss: 0.6200 - val_acc: 0.6794\n",
      "Epoch 36/100\n",
      "22777/22777 [==============================] - 2s 89us/sample - loss: 0.6292 - acc: 0.6653 - val_loss: 0.6453 - val_acc: 0.6479\n",
      "Epoch 37/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6307 - acc: 0.6644 - val_loss: 0.6211 - val_acc: 0.6793\n",
      "Epoch 38/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6316 - acc: 0.6623 - val_loss: 0.6209 - val_acc: 0.6782\n",
      "Epoch 39/100\n",
      "22777/22777 [==============================] - 2s 79us/sample - loss: 0.6298 - acc: 0.6663 - val_loss: 0.6199 - val_acc: 0.6792\n",
      "Epoch 40/100\n",
      "22777/22777 [==============================] - 2s 83us/sample - loss: 0.6291 - acc: 0.6639 - val_loss: 0.6209 - val_acc: 0.6750\n",
      "Epoch 41/100\n",
      "22777/22777 [==============================] - 1s 65us/sample - loss: 0.6300 - acc: 0.6644 - val_loss: 0.6209 - val_acc: 0.6777\n",
      "Epoch 42/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6290 - acc: 0.6655 - val_loss: 0.6230 - val_acc: 0.6705\n",
      "Epoch 43/100\n",
      "22777/22777 [==============================] - 2s 105us/sample - loss: 0.6297 - acc: 0.6623 - val_loss: 0.6233 - val_acc: 0.6716\n",
      "Epoch 44/100\n",
      "22777/22777 [==============================] - 2s 99us/sample - loss: 0.6285 - acc: 0.6663 - val_loss: 0.6256 - val_acc: 0.6695\n",
      "Epoch 45/100\n",
      "22777/22777 [==============================] - 2s 79us/sample - loss: 0.6279 - acc: 0.6690 - val_loss: 0.6246 - val_acc: 0.6676\n",
      "Epoch 46/100\n",
      "22777/22777 [==============================] - 2s 75us/sample - loss: 0.6287 - acc: 0.6629 - val_loss: 0.6192 - val_acc: 0.6794\n",
      "Epoch 47/100\n",
      "22777/22777 [==============================] - 2s 106us/sample - loss: 0.6292 - acc: 0.6657 - val_loss: 0.6278 - val_acc: 0.6645\n",
      "Epoch 48/100\n",
      "22777/22777 [==============================] - 2s 76us/sample - loss: 0.6289 - acc: 0.6647 - val_loss: 0.6500 - val_acc: 0.6331\n",
      "Epoch 49/100\n",
      "22777/22777 [==============================] - 2s 79us/sample - loss: 0.6298 - acc: 0.6610 - val_loss: 0.6486 - val_acc: 0.6462\n",
      "Epoch 50/100\n",
      "22777/22777 [==============================] - 2s 106us/sample - loss: 0.6297 - acc: 0.6669 - val_loss: 0.6209 - val_acc: 0.6767\n",
      "Epoch 51/100\n",
      "22777/22777 [==============================] - 2s 86us/sample - loss: 0.6285 - acc: 0.6629 - val_loss: 0.6204 - val_acc: 0.6760\n",
      "Epoch 52/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6281 - acc: 0.6657 - val_loss: 0.6208 - val_acc: 0.6749\n",
      "Epoch 53/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6309 - acc: 0.6642 - val_loss: 0.6198 - val_acc: 0.6790\n",
      "Epoch 54/100\n",
      "22777/22777 [==============================] - 2s 95us/sample - loss: 0.6279 - acc: 0.6639 - val_loss: 0.6205 - val_acc: 0.6760\n",
      "Epoch 55/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6295 - acc: 0.6638 - val_loss: 0.6341 - val_acc: 0.6529\n",
      "Epoch 56/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6278 - acc: 0.6632 - val_loss: 0.6401 - val_acc: 0.6497\n",
      "Epoch 57/100\n",
      "22777/22777 [==============================] - 2s 82us/sample - loss: 0.6289 - acc: 0.6655 - val_loss: 0.6205 - val_acc: 0.6706\n",
      "Epoch 58/100\n",
      "22777/22777 [==============================] - 2s 84us/sample - loss: 0.6281 - acc: 0.6676 - val_loss: 0.6199 - val_acc: 0.6781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6281 - acc: 0.6666 - val_loss: 0.6194 - val_acc: 0.68000s - loss: 0.6300 - acc\n",
      "Epoch 60/100\n",
      "22777/22777 [==============================] - 2s 81us/sample - loss: 0.6291 - acc: 0.6661 - val_loss: 0.6215 - val_acc: 0.6754\n",
      "Epoch 61/100\n",
      "22777/22777 [==============================] - 2s 98us/sample - loss: 0.6291 - acc: 0.6667 - val_loss: 0.6293 - val_acc: 0.6650\n",
      "Epoch 62/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6279 - acc: 0.6676 - val_loss: 0.6188 - val_acc: 0.6801\n",
      "Epoch 63/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6274 - acc: 0.6658 - val_loss: 0.6322 - val_acc: 0.6553\n",
      "Epoch 64/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6283 - acc: 0.6666 - val_loss: 0.6208 - val_acc: 0.6766\n",
      "Epoch 65/100\n",
      "22777/22777 [==============================] - 2s 101us/sample - loss: 0.6276 - acc: 0.6690 - val_loss: 0.6202 - val_acc: 0.6761\n",
      "Epoch 66/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6273 - acc: 0.6674 - val_loss: 0.6182 - val_acc: 0.6784\n",
      "Epoch 67/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6281 - acc: 0.6653 - val_loss: 0.6177 - val_acc: 0.6807\n",
      "Epoch 68/100\n",
      "22777/22777 [==============================] - 2s 72us/sample - loss: 0.6271 - acc: 0.6667 - val_loss: 0.6213 - val_acc: 0.6736\n",
      "Epoch 69/100\n",
      "22777/22777 [==============================] - 2s 107us/sample - loss: 0.6285 - acc: 0.6637 - val_loss: 0.6225 - val_acc: 0.6722\n",
      "Epoch 70/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6275 - acc: 0.6660 - val_loss: 0.6470 - val_acc: 0.6430\n",
      "Epoch 71/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6273 - acc: 0.6701 - val_loss: 0.6308 - val_acc: 0.6592\n",
      "Epoch 72/100\n",
      "22777/22777 [==============================] - 2s 91us/sample - loss: 0.6283 - acc: 0.6647 - val_loss: 0.6185 - val_acc: 0.6764\n",
      "Epoch 73/100\n",
      "22777/22777 [==============================] - 2s 72us/sample - loss: 0.6275 - acc: 0.6664 - val_loss: 0.6268 - val_acc: 0.6692\n",
      "Epoch 74/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6268 - acc: 0.6666 - val_loss: 0.6276 - val_acc: 0.6657\n",
      "Epoch 75/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6269 - acc: 0.6683 - val_loss: 0.6288 - val_acc: 0.6602\n",
      "Epoch 76/100\n",
      "22777/22777 [==============================] - 2s 105us/sample - loss: 0.6279 - acc: 0.6641 - val_loss: 0.6262 - val_acc: 0.6622\n",
      "Epoch 77/100\n",
      "22777/22777 [==============================] - 2s 89us/sample - loss: 0.6274 - acc: 0.6644 - val_loss: 0.6302 - val_acc: 0.6606\n",
      "Epoch 78/100\n",
      "22777/22777 [==============================] - 2s 86us/sample - loss: 0.6276 - acc: 0.6684 - val_loss: 0.6245 - val_acc: 0.6669\n",
      "Epoch 79/100\n",
      "22777/22777 [==============================] - 2s 100us/sample - loss: 0.6284 - acc: 0.6647 - val_loss: 0.6262 - val_acc: 0.6660\n",
      "Epoch 80/100\n",
      "22777/22777 [==============================] - 2s 72us/sample - loss: 0.6270 - acc: 0.6680 - val_loss: 0.6274 - val_acc: 0.6644\n",
      "Epoch 81/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6266 - acc: 0.6677 - val_loss: 0.6179 - val_acc: 0.6783\n",
      "Epoch 82/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6264 - acc: 0.6647 - val_loss: 0.6272 - val_acc: 0.6669\n",
      "Epoch 83/100\n",
      "22777/22777 [==============================] - 2s 100us/sample - loss: 0.6257 - acc: 0.6699 - val_loss: 0.6176 - val_acc: 0.6803\n",
      "Epoch 84/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6265 - acc: 0.6673 - val_loss: 0.6227 - val_acc: 0.6708\n",
      "Epoch 85/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6264 - acc: 0.6668 - val_loss: 0.6295 - val_acc: 0.6601\n",
      "Epoch 86/100\n",
      "22777/22777 [==============================] - 2s 79us/sample - loss: 0.6281 - acc: 0.6669 - val_loss: 0.6274 - val_acc: 0.6668\n",
      "Epoch 87/100\n",
      "22777/22777 [==============================] - 2s 102us/sample - loss: 0.6274 - acc: 0.6675 - val_loss: 0.6227 - val_acc: 0.6672\n",
      "Epoch 88/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6264 - acc: 0.6689 - val_loss: 0.6506 - val_acc: 0.6434\n",
      "Epoch 89/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6268 - acc: 0.6665 - val_loss: 0.6242 - val_acc: 0.6646\n",
      "Epoch 90/100\n",
      "22777/22777 [==============================] - 2s 88us/sample - loss: 0.6265 - acc: 0.6640 - val_loss: 0.6243 - val_acc: 0.6731\n",
      "Epoch 91/100\n",
      "22777/22777 [==============================] - 2s 75us/sample - loss: 0.6276 - acc: 0.6666 - val_loss: 0.6198 - val_acc: 0.6787\n",
      "Epoch 92/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6277 - acc: 0.6664 - val_loss: 0.6231 - val_acc: 0.6676\n",
      "Epoch 93/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6254 - acc: 0.6665 - val_loss: 0.6257 - val_acc: 0.66240s - loss: 0.6275 - \n",
      "Epoch 94/100\n",
      "22777/22777 [==============================] - 2s 106us/sample - loss: 0.6250 - acc: 0.6658 - val_loss: 0.6291 - val_acc: 0.6605\n",
      "Epoch 95/100\n",
      "22777/22777 [==============================] - 2s 76us/sample - loss: 0.6269 - acc: 0.6669 - val_loss: 0.6172 - val_acc: 0.6788\n",
      "Epoch 96/100\n",
      "22777/22777 [==============================] - 2s 75us/sample - loss: 0.6269 - acc: 0.6688 - val_loss: 0.6298 - val_acc: 0.6631\n",
      "Epoch 97/100\n",
      "22777/22777 [==============================] - 2s 86us/sample - loss: 0.6274 - acc: 0.6662 - val_loss: 0.6363 - val_acc: 0.6501\n",
      "Epoch 98/100\n",
      "22777/22777 [==============================] - 2s 85us/sample - loss: 0.6252 - acc: 0.6682 - val_loss: 0.6227 - val_acc: 0.6673\n",
      "Epoch 99/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6280 - acc: 0.6659 - val_loss: 0.6182 - val_acc: 0.67810s - loss: 0.6314\n",
      "Epoch 100/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6274 - acc: 0.6657 - val_loss: 0.6303 - val_acc: 0.6557\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 16)                2992      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,281\n",
      "Trainable params: 3,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train, loss and metric: [0.6609897050701203, 0.6295082]\n"
     ]
    }
   ],
   "source": [
    "# L2규제만\n",
    "l2_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                       activation=tf.nn.relu, input_shape=(186,)),\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                       activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "\n",
    "l2_model.compile(optimizer='sgd',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "l2_model_history = l2_model.fit(xData, yData,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(xData, yData),\n",
    "                                verbose=1)\n",
    "\n",
    "l2_model.summary()\n",
    "loss_and_metric = l2_model.evaluate(testX, testY, batch_size=32, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22777 samples, validate on 610 samples\n",
      "Epoch 1/100\n",
      "22777/22777 [==============================] - 2s 80us/sample - loss: 0.6835 - acc: 0.5660 - val_loss: 0.6792 - val_acc: 0.5754\n",
      "Epoch 2/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6676 - acc: 0.5944 - val_loss: 0.6756 - val_acc: 0.5852\n",
      "Epoch 3/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6618 - acc: 0.6099 - val_loss: 0.6607 - val_acc: 0.6033\n",
      "Epoch 4/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6524 - acc: 0.6233 - val_loss: 0.6544 - val_acc: 0.6049\n",
      "Epoch 5/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6455 - acc: 0.6371 - val_loss: 0.6527 - val_acc: 0.6066\n",
      "Epoch 6/100\n",
      "22777/22777 [==============================] - 1s 57us/sample - loss: 0.6439 - acc: 0.6406 - val_loss: 0.6490 - val_acc: 0.6213\n",
      "Epoch 7/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6386 - acc: 0.6453 - val_loss: 0.6447 - val_acc: 0.6131\n",
      "Epoch 8/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6393 - acc: 0.6443 - val_loss: 0.6431 - val_acc: 0.6311\n",
      "Epoch 9/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6373 - acc: 0.6452 - val_loss: 0.6457 - val_acc: 0.6279\n",
      "Epoch 10/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6341 - acc: 0.6483 - val_loss: 0.6423 - val_acc: 0.6148\n",
      "Epoch 11/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6341 - acc: 0.6463 - val_loss: 0.6483 - val_acc: 0.6262\n",
      "Epoch 12/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6313 - acc: 0.6489 - val_loss: 0.6437 - val_acc: 0.6311\n",
      "Epoch 13/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6293 - acc: 0.6537 - val_loss: 0.6448 - val_acc: 0.6361\n",
      "Epoch 14/100\n",
      "22777/22777 [==============================] - 1s 54us/sample - loss: 0.6284 - acc: 0.6509 - val_loss: 0.6369 - val_acc: 0.6262\n",
      "Epoch 15/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6255 - acc: 0.6539 - val_loss: 0.6403 - val_acc: 0.6328\n",
      "Epoch 16/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6246 - acc: 0.6552 - val_loss: 0.6356 - val_acc: 0.6295\n",
      "Epoch 17/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6246 - acc: 0.6553 - val_loss: 0.6348 - val_acc: 0.6525\n",
      "Epoch 18/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6234 - acc: 0.6597 - val_loss: 0.6558 - val_acc: 0.6311\n",
      "Epoch 19/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6195 - acc: 0.6611 - val_loss: 0.6341 - val_acc: 0.6410\n",
      "Epoch 20/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6211 - acc: 0.6598 - val_loss: 0.6371 - val_acc: 0.6344\n",
      "Epoch 21/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6205 - acc: 0.6590 - val_loss: 0.6403 - val_acc: 0.6180\n",
      "Epoch 22/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6196 - acc: 0.6605 - val_loss: 0.6394 - val_acc: 0.6393\n",
      "Epoch 23/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6187 - acc: 0.6576 - val_loss: 0.6406 - val_acc: 0.6475\n",
      "Epoch 24/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6169 - acc: 0.6605 - val_loss: 0.6339 - val_acc: 0.6115\n",
      "Epoch 25/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6156 - acc: 0.6633 - val_loss: 0.6337 - val_acc: 0.6328\n",
      "Epoch 26/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6161 - acc: 0.6654 - val_loss: 0.6369 - val_acc: 0.6459\n",
      "Epoch 27/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6165 - acc: 0.6633 - val_loss: 0.6371 - val_acc: 0.6328\n",
      "Epoch 28/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6163 - acc: 0.6663 - val_loss: 0.6372 - val_acc: 0.6393\n",
      "Epoch 29/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6162 - acc: 0.6615 - val_loss: 0.6418 - val_acc: 0.6344\n",
      "Epoch 30/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6137 - acc: 0.6654 - val_loss: 0.6383 - val_acc: 0.6295\n",
      "Epoch 31/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6160 - acc: 0.6648 - val_loss: 0.6365 - val_acc: 0.6344\n",
      "Epoch 32/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6146 - acc: 0.6645 - val_loss: 0.6342 - val_acc: 0.6328\n",
      "Epoch 33/100\n",
      "22777/22777 [==============================] - 2s 90us/sample - loss: 0.6142 - acc: 0.6655 - val_loss: 0.6465 - val_acc: 0.6475\n",
      "Epoch 34/100\n",
      "22777/22777 [==============================] - 2s 69us/sample - loss: 0.6132 - acc: 0.6629 - val_loss: 0.6332 - val_acc: 0.6426\n",
      "Epoch 35/100\n",
      "22777/22777 [==============================] - 2s 75us/sample - loss: 0.6142 - acc: 0.6691 - val_loss: 0.6561 - val_acc: 0.6279\n",
      "Epoch 36/100\n",
      "22777/22777 [==============================] - 2s 73us/sample - loss: 0.6133 - acc: 0.6676 - val_loss: 0.6379 - val_acc: 0.6328\n",
      "Epoch 37/100\n",
      "22777/22777 [==============================] - 2s 87us/sample - loss: 0.6133 - acc: 0.6652 - val_loss: 0.6398 - val_acc: 0.6426\n",
      "Epoch 38/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6137 - acc: 0.6641 - val_loss: 0.6340 - val_acc: 0.6492\n",
      "Epoch 39/100\n",
      "22777/22777 [==============================] - 2s 77us/sample - loss: 0.6122 - acc: 0.6687 - val_loss: 0.6340 - val_acc: 0.6246\n",
      "Epoch 40/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6123 - acc: 0.6664 - val_loss: 0.6341 - val_acc: 0.6361\n",
      "Epoch 41/100\n",
      "22777/22777 [==============================] - 2s 80us/sample - loss: 0.6121 - acc: 0.6669 - val_loss: 0.6452 - val_acc: 0.6443\n",
      "Epoch 42/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6128 - acc: 0.6652 - val_loss: 0.6431 - val_acc: 0.6525\n",
      "Epoch 43/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6119 - acc: 0.6683 - val_loss: 0.6409 - val_acc: 0.6328\n",
      "Epoch 44/100\n",
      "22777/22777 [==============================] - 1s 59us/sample - loss: 0.6123 - acc: 0.6670 - val_loss: 0.6342 - val_acc: 0.6311\n",
      "Epoch 45/100\n",
      "22777/22777 [==============================] - 2s 76us/sample - loss: 0.6103 - acc: 0.6679 - val_loss: 0.6346 - val_acc: 0.6377\n",
      "Epoch 46/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6119 - acc: 0.6681 - val_loss: 0.6414 - val_acc: 0.6344\n",
      "Epoch 47/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6113 - acc: 0.6653 - val_loss: 0.6311 - val_acc: 0.6426\n",
      "Epoch 48/100\n",
      "22777/22777 [==============================] - 1s 65us/sample - loss: 0.6106 - acc: 0.6708 - val_loss: 0.6362 - val_acc: 0.6295\n",
      "Epoch 49/100\n",
      "22777/22777 [==============================] - 1s 50us/sample - loss: 0.6099 - acc: 0.6696 - val_loss: 0.6463 - val_acc: 0.6361\n",
      "Epoch 50/100\n",
      "22777/22777 [==============================] - 2s 79us/sample - loss: 0.6115 - acc: 0.6691 - val_loss: 0.6514 - val_acc: 0.6279\n",
      "Epoch 51/100\n",
      "22777/22777 [==============================] - 2s 80us/sample - loss: 0.6117 - acc: 0.6662 - val_loss: 0.6444 - val_acc: 0.6311\n",
      "Epoch 52/100\n",
      "22777/22777 [==============================] - 2s 81us/sample - loss: 0.6107 - acc: 0.6642 - val_loss: 0.6283 - val_acc: 0.6541\n",
      "Epoch 53/100\n",
      "22777/22777 [==============================] - 2s 74us/sample - loss: 0.6105 - acc: 0.6665 - val_loss: 0.6524 - val_acc: 0.6475\n",
      "Epoch 54/100\n",
      "22777/22777 [==============================] - 2s 74us/sample - loss: 0.6088 - acc: 0.6724 - val_loss: 0.6346 - val_acc: 0.6508\n",
      "Epoch 55/100\n",
      "22777/22777 [==============================] - 1s 57us/sample - loss: 0.6114 - acc: 0.6666 - val_loss: 0.6505 - val_acc: 0.5984\n",
      "Epoch 56/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6105 - acc: 0.6673 - val_loss: 0.6431 - val_acc: 0.6426\n",
      "Epoch 57/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6097 - acc: 0.6706 - val_loss: 0.6366 - val_acc: 0.6459\n",
      "Epoch 58/100\n",
      "22777/22777 [==============================] - 1s 50us/sample - loss: 0.6101 - acc: 0.6670 - val_loss: 0.6383 - val_acc: 0.6311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "22777/22777 [==============================] - 2s 71us/sample - loss: 0.6099 - acc: 0.6698 - val_loss: 0.6337 - val_acc: 0.6328\n",
      "Epoch 60/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6125 - acc: 0.6678 - val_loss: 0.6662 - val_acc: 0.6262\n",
      "Epoch 61/100\n",
      "22777/22777 [==============================] - 1s 57us/sample - loss: 0.6081 - acc: 0.6720 - val_loss: 0.6315 - val_acc: 0.6426\n",
      "Epoch 62/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6085 - acc: 0.6701 - val_loss: 0.6449 - val_acc: 0.6311\n",
      "Epoch 63/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6087 - acc: 0.6713 - val_loss: 0.6390 - val_acc: 0.6377\n",
      "Epoch 64/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6094 - acc: 0.6708 - val_loss: 0.6348 - val_acc: 0.6328\n",
      "Epoch 65/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6096 - acc: 0.6700 - val_loss: 0.6397 - val_acc: 0.6492\n",
      "Epoch 66/100\n",
      "22777/22777 [==============================] - 1s 53us/sample - loss: 0.6086 - acc: 0.6703 - val_loss: 0.6345 - val_acc: 0.6459\n",
      "Epoch 67/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6061 - acc: 0.6723 - val_loss: 0.6356 - val_acc: 0.6459\n",
      "Epoch 68/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6078 - acc: 0.6734 - val_loss: 0.6381 - val_acc: 0.6246\n",
      "Epoch 69/100\n",
      "22777/22777 [==============================] - 1s 54us/sample - loss: 0.6085 - acc: 0.6723 - val_loss: 0.6353 - val_acc: 0.6311\n",
      "Epoch 70/100\n",
      "22777/22777 [==============================] - 2s 72us/sample - loss: 0.6075 - acc: 0.6692 - val_loss: 0.6431 - val_acc: 0.6246\n",
      "Epoch 71/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6091 - acc: 0.6721 - val_loss: 0.6418 - val_acc: 0.6344\n",
      "Epoch 72/100\n",
      "22777/22777 [==============================] - 2s 74us/sample - loss: 0.6088 - acc: 0.6697 - val_loss: 0.6366 - val_acc: 0.6361\n",
      "Epoch 73/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6066 - acc: 0.6715 - val_loss: 0.6430 - val_acc: 0.6279\n",
      "Epoch 74/100\n",
      "22777/22777 [==============================] - 2s 76us/sample - loss: 0.6079 - acc: 0.6721 - val_loss: 0.6389 - val_acc: 0.6410\n",
      "Epoch 75/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6085 - acc: 0.6695 - val_loss: 0.6365 - val_acc: 0.6279\n",
      "Epoch 76/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6077 - acc: 0.6714 - val_loss: 0.6521 - val_acc: 0.6213\n",
      "Epoch 77/100\n",
      "22777/22777 [==============================] - 1s 51us/sample - loss: 0.6086 - acc: 0.6690 - val_loss: 0.6303 - val_acc: 0.6377\n",
      "Epoch 78/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6070 - acc: 0.6744 - val_loss: 0.6289 - val_acc: 0.6475\n",
      "Epoch 79/100\n",
      "22777/22777 [==============================] - 2s 78us/sample - loss: 0.6074 - acc: 0.6729 - val_loss: 0.6404 - val_acc: 0.6410\n",
      "Epoch 80/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6090 - acc: 0.6688 - val_loss: 0.6338 - val_acc: 0.6393\n",
      "Epoch 81/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6065 - acc: 0.6698 - val_loss: 0.6328 - val_acc: 0.6492\n",
      "Epoch 82/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6092 - acc: 0.6722 - val_loss: 0.6599 - val_acc: 0.6262\n",
      "Epoch 83/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6083 - acc: 0.6701 - val_loss: 0.6460 - val_acc: 0.6410\n",
      "Epoch 84/100\n",
      "22777/22777 [==============================] - 2s 78us/sample - loss: 0.6070 - acc: 0.6704 - val_loss: 0.6368 - val_acc: 0.6328\n",
      "Epoch 85/100\n",
      "22777/22777 [==============================] - 1s 60us/sample - loss: 0.6062 - acc: 0.6678 - val_loss: 0.6336 - val_acc: 0.6295\n",
      "Epoch 86/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6076 - acc: 0.6731 - val_loss: 0.6344 - val_acc: 0.6410\n",
      "Epoch 87/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6063 - acc: 0.6720 - val_loss: 0.6378 - val_acc: 0.6410\n",
      "Epoch 88/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6071 - acc: 0.6715 - val_loss: 0.6332 - val_acc: 0.6328\n",
      "Epoch 89/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6044 - acc: 0.6739 - val_loss: 0.6329 - val_acc: 0.6459\n",
      "Epoch 90/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6064 - acc: 0.6747 - val_loss: 0.6345 - val_acc: 0.6426\n",
      "Epoch 91/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6062 - acc: 0.6734 - val_loss: 0.6310 - val_acc: 0.6328\n",
      "Epoch 92/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6086 - acc: 0.6698 - val_loss: 0.6346 - val_acc: 0.6328\n",
      "Epoch 93/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6073 - acc: 0.6734 - val_loss: 0.6350 - val_acc: 0.6459\n",
      "Epoch 94/100\n",
      "22777/22777 [==============================] - 1s 62us/sample - loss: 0.6065 - acc: 0.6726 - val_loss: 0.6322 - val_acc: 0.6393\n",
      "Epoch 95/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6070 - acc: 0.6729 - val_loss: 0.6308 - val_acc: 0.6361\n",
      "Epoch 96/100\n",
      "22777/22777 [==============================] - 1s 61us/sample - loss: 0.6080 - acc: 0.6725 - val_loss: 0.6344 - val_acc: 0.6426\n",
      "Epoch 97/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6065 - acc: 0.6704 - val_loss: 0.6395 - val_acc: 0.6311\n",
      "Epoch 98/100\n",
      "22777/22777 [==============================] - 1s 51us/sample - loss: 0.6075 - acc: 0.6713 - val_loss: 0.6330 - val_acc: 0.6541\n",
      "Epoch 99/100\n",
      "22777/22777 [==============================] - 1s 66us/sample - loss: 0.6051 - acc: 0.6758 - val_loss: 0.6423 - val_acc: 0.6311\n",
      "Epoch 100/100\n",
      "22777/22777 [==============================] - 2s 66us/sample - loss: 0.6064 - acc: 0.6735 - val_loss: 0.6470 - val_acc: 0.6295\n",
      "train, loss and metric: [0.6470461356835288, 0.6295082]\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃만\n",
    "\n",
    "dpt_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(186,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "\n",
    "# l2_model.compile(optimizer='sgd',\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "\n",
    "dpt_model.compile(optimizer='sgd',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "dpt_model_history = dpt_model.fit(xData, yData,\n",
    "                                  epochs=100,\n",
    "                                  batch_size=32,\n",
    "                                  validation_data=(xData, yData),\n",
    "                                  verbose=1)\n",
    "\n",
    "loss_and_metric = dpt_model.evaluate(testX, testY, batch_size=32, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22777 samples, validate on 22777 samples\n",
      "Epoch 1/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6213 - acc: 0.6727 - val_loss: 0.6217 - val_acc: 0.6682\n",
      "Epoch 2/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6221 - acc: 0.6721 - val_loss: 0.6161 - val_acc: 0.6786\n",
      "Epoch 3/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6222 - acc: 0.6705 - val_loss: 0.6197 - val_acc: 0.6772\n",
      "Epoch 4/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6222 - acc: 0.6729 - val_loss: 0.6146 - val_acc: 0.6822\n",
      "Epoch 5/100\n",
      "22777/22777 [==============================] - 1s 63us/sample - loss: 0.6216 - acc: 0.6710 - val_loss: 0.6190 - val_acc: 0.6712\n",
      "Epoch 6/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6211 - acc: 0.6737 - val_loss: 0.6146 - val_acc: 0.6801\n",
      "Epoch 7/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6213 - acc: 0.6705 - val_loss: 0.6147 - val_acc: 0.6829\n",
      "Epoch 8/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6207 - acc: 0.6709 - val_loss: 0.6258 - val_acc: 0.6654\n",
      "Epoch 9/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6219 - acc: 0.6696 - val_loss: 0.6172 - val_acc: 0.6748\n",
      "Epoch 10/100\n",
      "22777/22777 [==============================] - 1s 50us/sample - loss: 0.6225 - acc: 0.6701 - val_loss: 0.6163 - val_acc: 0.6803\n",
      "Epoch 11/100\n",
      "22777/22777 [==============================] - 2s 70us/sample - loss: 0.6234 - acc: 0.6691 - val_loss: 0.6177 - val_acc: 0.6731\n",
      "Epoch 12/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6208 - acc: 0.6726 - val_loss: 0.6322 - val_acc: 0.6568\n",
      "Epoch 13/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6226 - acc: 0.6730 - val_loss: 0.6254 - val_acc: 0.6633\n",
      "Epoch 14/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6211 - acc: 0.6709 - val_loss: 0.6174 - val_acc: 0.6770\n",
      "Epoch 15/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6210 - acc: 0.6708 - val_loss: 0.6251 - val_acc: 0.6668\n",
      "Epoch 16/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6221 - acc: 0.6701 - val_loss: 0.6147 - val_acc: 0.6830\n",
      "Epoch 17/100\n",
      "22777/22777 [==============================] - 1s 64us/sample - loss: 0.6215 - acc: 0.6692 - val_loss: 0.6303 - val_acc: 0.6569\n",
      "Epoch 18/100\n",
      "22777/22777 [==============================] - 1s 53us/sample - loss: 0.6223 - acc: 0.6682 - val_loss: 0.6337 - val_acc: 0.6547\n",
      "Epoch 19/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6216 - acc: 0.6743 - val_loss: 0.6142 - val_acc: 0.6798\n",
      "Epoch 20/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6208 - acc: 0.6719 - val_loss: 0.6305 - val_acc: 0.6612\n",
      "Epoch 21/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6217 - acc: 0.6696 - val_loss: 0.6151 - val_acc: 0.6816\n",
      "Epoch 22/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6215 - acc: 0.6708 - val_loss: 0.6195 - val_acc: 0.6730\n",
      "Epoch 23/100\n",
      "22777/22777 [==============================] - 1s 60us/sample - loss: 0.6215 - acc: 0.6716 - val_loss: 0.6169 - val_acc: 0.6792\n",
      "Epoch 24/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6224 - acc: 0.6705 - val_loss: 0.6174 - val_acc: 0.6716\n",
      "Epoch 25/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6222 - acc: 0.6714 - val_loss: 0.6168 - val_acc: 0.6753\n",
      "Epoch 26/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6217 - acc: 0.6706 - val_loss: 0.6193 - val_acc: 0.6738\n",
      "Epoch 27/100\n",
      "22777/22777 [==============================] - 1s 49us/sample - loss: 0.6222 - acc: 0.6683 - val_loss: 0.6153 - val_acc: 0.6788\n",
      "Epoch 28/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6202 - acc: 0.6735 - val_loss: 0.6142 - val_acc: 0.6825\n",
      "Epoch 29/100\n",
      "22777/22777 [==============================] - 2s 77us/sample - loss: 0.6199 - acc: 0.6769 - val_loss: 0.6192 - val_acc: 0.6719\n",
      "Epoch 30/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6231 - acc: 0.6720 - val_loss: 0.6307 - val_acc: 0.6622\n",
      "Epoch 31/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6210 - acc: 0.6698 - val_loss: 0.6150 - val_acc: 0.6789\n",
      "Epoch 32/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6211 - acc: 0.6710 - val_loss: 0.6135 - val_acc: 0.6807\n",
      "Epoch 33/100\n",
      "22777/22777 [==============================] - 1s 51us/sample - loss: 0.6209 - acc: 0.6704 - val_loss: 0.6172 - val_acc: 0.6757\n",
      "Epoch 34/100\n",
      "22777/22777 [==============================] - 2s 68us/sample - loss: 0.6213 - acc: 0.6729 - val_loss: 0.6140 - val_acc: 0.6806\n",
      "Epoch 35/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6227 - acc: 0.6700 - val_loss: 0.6143 - val_acc: 0.6787\n",
      "Epoch 36/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6201 - acc: 0.6727 - val_loss: 0.6192 - val_acc: 0.6770\n",
      "Epoch 37/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6205 - acc: 0.6710 - val_loss: 0.6181 - val_acc: 0.6737\n",
      "Epoch 38/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6218 - acc: 0.6719 - val_loss: 0.6147 - val_acc: 0.6794\n",
      "Epoch 39/100\n",
      "22777/22777 [==============================] - 1s 50us/sample - loss: 0.6215 - acc: 0.6701 - val_loss: 0.6175 - val_acc: 0.6756\n",
      "Epoch 40/100\n",
      "22777/22777 [==============================] - 1s 59us/sample - loss: 0.6208 - acc: 0.6712 - val_loss: 0.6205 - val_acc: 0.6692\n",
      "Epoch 41/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6216 - acc: 0.6705 - val_loss: 0.6151 - val_acc: 0.6793\n",
      "Epoch 42/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6224 - acc: 0.6715 - val_loss: 0.6169 - val_acc: 0.6781\n",
      "Epoch 43/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6200 - acc: 0.6732 - val_loss: 0.6552 - val_acc: 0.6394\n",
      "Epoch 44/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6217 - acc: 0.6703 - val_loss: 0.6147 - val_acc: 0.6813\n",
      "Epoch 45/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6213 - acc: 0.6730 - val_loss: 0.6209 - val_acc: 0.6694\n",
      "Epoch 46/100\n",
      "22777/22777 [==============================] - 2s 67us/sample - loss: 0.6213 - acc: 0.6703 - val_loss: 0.6258 - val_acc: 0.6612\n",
      "Epoch 47/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6200 - acc: 0.6724 - val_loss: 0.6169 - val_acc: 0.6772\n",
      "Epoch 48/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6228 - acc: 0.6721 - val_loss: 0.6184 - val_acc: 0.6745\n",
      "Epoch 49/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6225 - acc: 0.6687 - val_loss: 0.6202 - val_acc: 0.6736\n",
      "Epoch 50/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6219 - acc: 0.6727 - val_loss: 0.6260 - val_acc: 0.6625\n",
      "Epoch 51/100\n",
      "22777/22777 [==============================] - 1s 44us/sample - loss: 0.6215 - acc: 0.6719 - val_loss: 0.6264 - val_acc: 0.6603\n",
      "Epoch 52/100\n",
      "22777/22777 [==============================] - 1s 59us/sample - loss: 0.6215 - acc: 0.6713 - val_loss: 0.6195 - val_acc: 0.6725\n",
      "Epoch 53/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6211 - acc: 0.6719 - val_loss: 0.6142 - val_acc: 0.6806\n",
      "Epoch 54/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6209 - acc: 0.6730 - val_loss: 0.6333 - val_acc: 0.6587\n",
      "Epoch 55/100\n",
      "22777/22777 [==============================] - 1s 40us/sample - loss: 0.6220 - acc: 0.6707 - val_loss: 0.6144 - val_acc: 0.6773\n",
      "Epoch 56/100\n",
      "22777/22777 [==============================] - 1s 48us/sample - loss: 0.6208 - acc: 0.6709 - val_loss: 0.6133 - val_acc: 0.6806\n",
      "Epoch 57/100\n",
      "22777/22777 [==============================] - 1s 45us/sample - loss: 0.6199 - acc: 0.6745 - val_loss: 0.6129 - val_acc: 0.6803\n",
      "Epoch 58/100\n",
      "22777/22777 [==============================] - 1s 56us/sample - loss: 0.6222 - acc: 0.6695 - val_loss: 0.6162 - val_acc: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6217 - acc: 0.6727 - val_loss: 0.6219 - val_acc: 0.6673\n",
      "Epoch 60/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6207 - acc: 0.6740 - val_loss: 0.6159 - val_acc: 0.6748\n",
      "Epoch 61/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6201 - acc: 0.6734 - val_loss: 0.6141 - val_acc: 0.6795\n",
      "Epoch 62/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6210 - acc: 0.6716 - val_loss: 0.6154 - val_acc: 0.6805\n",
      "Epoch 63/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6212 - acc: 0.6723 - val_loss: 0.6165 - val_acc: 0.6780\n",
      "Epoch 64/100\n",
      "22777/22777 [==============================] - 1s 58us/sample - loss: 0.6209 - acc: 0.6701 - val_loss: 0.6259 - val_acc: 0.6626\n",
      "Epoch 65/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6218 - acc: 0.6719 - val_loss: 0.6475 - val_acc: 0.6486\n",
      "Epoch 66/100\n",
      "22777/22777 [==============================] - 1s 38us/sample - loss: 0.6213 - acc: 0.6690 - val_loss: 0.6327 - val_acc: 0.6520\n",
      "Epoch 67/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6210 - acc: 0.6702 - val_loss: 0.6132 - val_acc: 0.6828\n",
      "Epoch 68/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6219 - acc: 0.6701 - val_loss: 0.6208 - val_acc: 0.6669\n",
      "Epoch 69/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6207 - acc: 0.6707 - val_loss: 0.6191 - val_acc: 0.6708\n",
      "Epoch 70/100\n",
      "22777/22777 [==============================] - 1s 52us/sample - loss: 0.6226 - acc: 0.6692 - val_loss: 0.6155 - val_acc: 0.6780\n",
      "Epoch 71/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6222 - acc: 0.6705 - val_loss: 0.6154 - val_acc: 0.6806\n",
      "Epoch 72/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6203 - acc: 0.6706 - val_loss: 0.6150 - val_acc: 0.6799\n",
      "Epoch 73/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6200 - acc: 0.6726 - val_loss: 0.6174 - val_acc: 0.6810\n",
      "Epoch 74/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6204 - acc: 0.6704 - val_loss: 0.6161 - val_acc: 0.6796\n",
      "Epoch 75/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6194 - acc: 0.6754 - val_loss: 0.6710 - val_acc: 0.6211\n",
      "Epoch 76/100\n",
      "22777/22777 [==============================] - 1s 53us/sample - loss: 0.6210 - acc: 0.6716 - val_loss: 0.6149 - val_acc: 0.6818\n",
      "Epoch 77/100\n",
      "22777/22777 [==============================] - 1s 57us/sample - loss: 0.6221 - acc: 0.6704 - val_loss: 0.6146 - val_acc: 0.6806\n",
      "Epoch 78/100\n",
      "22777/22777 [==============================] - 1s 38us/sample - loss: 0.6200 - acc: 0.6708 - val_loss: 0.6139 - val_acc: 0.6813\n",
      "Epoch 79/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6208 - acc: 0.6719 - val_loss: 0.6135 - val_acc: 0.6815\n",
      "Epoch 80/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6205 - acc: 0.6719 - val_loss: 0.6258 - val_acc: 0.6637\n",
      "Epoch 81/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6217 - acc: 0.6703 - val_loss: 0.6205 - val_acc: 0.6725\n",
      "Epoch 82/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6204 - acc: 0.6714 - val_loss: 0.6175 - val_acc: 0.6777\n",
      "Epoch 83/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6199 - acc: 0.6731 - val_loss: 0.6158 - val_acc: 0.6792\n",
      "Epoch 84/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6184 - acc: 0.6760 - val_loss: 0.6301 - val_acc: 0.6591\n",
      "Epoch 85/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6213 - acc: 0.6732 - val_loss: 0.6149 - val_acc: 0.6810\n",
      "Epoch 86/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6221 - acc: 0.6691 - val_loss: 0.6181 - val_acc: 0.6738\n",
      "Epoch 87/100\n",
      "22777/22777 [==============================] - 1s 43us/sample - loss: 0.6210 - acc: 0.6699 - val_loss: 0.6185 - val_acc: 0.6719\n",
      "Epoch 88/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6210 - acc: 0.6718 - val_loss: 0.6420 - val_acc: 0.6467\n",
      "Epoch 89/100\n",
      "22777/22777 [==============================] - 1s 59us/sample - loss: 0.6218 - acc: 0.6715 - val_loss: 0.6186 - val_acc: 0.6709\n",
      "Epoch 90/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6211 - acc: 0.6716 - val_loss: 0.6187 - val_acc: 0.6688\n",
      "Epoch 91/100\n",
      "22777/22777 [==============================] - 1s 39us/sample - loss: 0.6207 - acc: 0.6687 - val_loss: 0.6348 - val_acc: 0.6553\n",
      "Epoch 92/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6199 - acc: 0.6739 - val_loss: 0.6192 - val_acc: 0.6752\n",
      "Epoch 93/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6211 - acc: 0.6717 - val_loss: 0.6259 - val_acc: 0.6664\n",
      "Epoch 94/100\n",
      "22777/22777 [==============================] - 1s 41us/sample - loss: 0.6207 - acc: 0.6710 - val_loss: 0.6229 - val_acc: 0.6660\n",
      "Epoch 95/100\n",
      "22777/22777 [==============================] - 1s 59us/sample - loss: 0.6201 - acc: 0.6696 - val_loss: 0.6191 - val_acc: 0.6745\n",
      "Epoch 96/100\n",
      "22777/22777 [==============================] - 1s 42us/sample - loss: 0.6227 - acc: 0.6688 - val_loss: 0.6134 - val_acc: 0.6838\n",
      "Epoch 97/100\n",
      "22777/22777 [==============================] - 1s 46us/sample - loss: 0.6206 - acc: 0.6721 - val_loss: 0.6181 - val_acc: 0.6752\n",
      "Epoch 98/100\n",
      "22777/22777 [==============================] - 1s 57us/sample - loss: 0.6200 - acc: 0.6711 - val_loss: 0.6506 - val_acc: 0.6512\n",
      "Epoch 99/100\n",
      "22777/22777 [==============================] - 1s 47us/sample - loss: 0.6221 - acc: 0.6713 - val_loss: 0.6189 - val_acc: 0.6741\n",
      "Epoch 100/100\n",
      "22777/22777 [==============================] - 1s 55us/sample - loss: 0.6203 - acc: 0.6708 - val_loss: 0.6127 - val_acc: 0.6824\n",
      "train, loss and metric: [1.2109216314847353, 0.49508196]\n"
     ]
    }
   ],
   "source": [
    "# L2규제, 드롭아웃 모두 적용\n",
    "\n",
    "l2_dpt_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                       activation=tf.nn.relu, input_shape=(186,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                       activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "]) \n",
    "adam=optimizers.Adam(lr=0.01)\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "l2_dpt_model.compile(optimizer=sgd,\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "l2_model.fit(xData, yData,epochs=100,batch_size=64,validation_data=(xData, yData),verbose=1)\n",
    "loss_and_metric = l2_dpt_model.evaluate(testX, testY, batch_size=32, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입양 확률 :  94.2203%\n",
      "입양 확률 :  36.7390%\n",
      "입양 확률 :  35.8996%\n",
      "입양 확률 :  78.0932%\n",
      "입양 확률 :  21.0893%\n",
      "입양 확률 :  21.0893%\n",
      "입양 확률 :  91.3862%\n",
      "입양 확률 :  78.4416%\n",
      "입양 확률 :  89.5666%\n",
      "입양 확률 :  84.8468%\n",
      "입양 확률 :  72.6395%\n",
      "입양 확률 :  85.3167%\n",
      "입양 확률 :  55.5937%\n",
      "입양 확률 :  49.5257%\n",
      "입양 확률 :  51.3644%\n",
      "입양 확률 :  28.9706%\n",
      "입양 확률 :  44.0449%\n",
      "입양 확률 :  37.9210%\n",
      "입양 확률 :  83.8701%\n",
      "입양 확률 :  23.1563%\n",
      "입양 확률 :  64.8517%\n",
      "입양 확률 :  64.8517%\n",
      "입양 확률 :  53.8228%\n",
      "입양 확률 :  46.9142%\n",
      "입양 확률 :  71.4867%\n",
      "입양 확률 :  58.0059%\n",
      "입양 확률 :  54.1604%\n",
      "입양 확률 :  57.8769%\n",
      "입양 확률 :  41.1052%\n",
      "입양 확률 :  54.8027%\n",
      "입양 확률 :  56.1087%\n",
      "입양 확률 :  41.1052%\n",
      "입양 확률 :  40.0840%\n",
      "입양 확률 :  37.2096%\n",
      "입양 확률 :  90.8167%\n",
      "입양 확률 :  88.6520%\n",
      "입양 확률 :  19.2490%\n",
      "입양 확률 :  69.2019%\n",
      "입양 확률 :  76.6373%\n",
      "입양 확률 :  28.4616%\n",
      "입양 확률 :  26.2082%\n",
      "입양 확률 :  37.8162%\n",
      "입양 확률 :  49.3927%\n",
      "입양 확률 :  31.0500%\n",
      "입양 확률 :  32.5425%\n",
      "입양 확률 :  45.5628%\n",
      "입양 확률 :  49.3927%\n",
      "입양 확률 :  51.3507%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  45.9654%\n",
      "입양 확률 :  29.5362%\n",
      "입양 확률 :  29.1457%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  52.2923%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  51.3507%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  32.1351%\n",
      "입양 확률 :  67.2334%\n",
      "입양 확률 :  56.3661%\n",
      "입양 확률 :  57.1026%\n",
      "입양 확률 :  55.8354%\n",
      "입양 확률 :  37.6229%\n",
      "입양 확률 :  93.1751%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  78.7943%\n",
      "입양 확률 :  18.4336%\n",
      "입양 확률 :  80.2123%\n",
      "입양 확률 :  72.8484%\n",
      "입양 확률 :  51.0164%\n",
      "입양 확률 :  53.1521%\n",
      "입양 확률 :  77.7039%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  37.9588%\n",
      "입양 확률 :  42.8758%\n",
      "입양 확률 :  51.4375%\n",
      "입양 확률 :  34.8071%\n",
      "입양 확률 :  90.5109%\n",
      "입양 확률 :  23.0991%\n",
      "입양 확률 :  74.9863%\n",
      "입양 확률 :  48.8126%\n",
      "입양 확률 :  52.8710%\n",
      "입양 확률 :  48.1297%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.9207%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  23.6678%\n",
      "입양 확률 :  75.7269%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  57.5152%\n",
      "입양 확률 :  50.9764%\n",
      "입양 확률 :  90.9335%\n",
      "입양 확률 :  34.4362%\n",
      "입양 확률 :  80.4421%\n",
      "입양 확률 :  86.2377%\n",
      "입양 확률 :  24.7418%\n",
      "입양 확률 :  62.9177%\n",
      "입양 확률 :  85.2348%\n",
      "입양 확률 :  28.6270%\n",
      "입양 확률 :  84.8854%\n",
      "입양 확률 :  65.3681%\n",
      "입양 확률 :  29.6700%\n",
      "입양 확률 :  42.5086%\n",
      "입양 확률 :  71.8598%\n",
      "입양 확률 :  37.7539%\n",
      "입양 확률 :  32.4765%\n",
      "입양 확률 :  38.1661%\n",
      "입양 확률 :  86.8071%\n",
      "입양 확률 :  79.7014%\n",
      "입양 확률 :  22.1511%\n",
      "입양 확률 :  22.1511%\n",
      "입양 확률 :  81.6765%\n",
      "입양 확률 :  26.6789%\n",
      "입양 확률 :  65.5374%\n",
      "입양 확률 :  48.4780%\n",
      "입양 확률 :  63.2604%\n",
      "입양 확률 :  89.2275%\n",
      "입양 확률 :  30.8939%\n",
      "입양 확률 :  37.8441%\n",
      "입양 확률 :  38.0527%\n",
      "입양 확률 :  29.3965%\n",
      "입양 확률 :  76.1217%\n",
      "입양 확률 :  83.7344%\n",
      "입양 확률 :  82.3084%\n",
      "입양 확률 :  35.1551%\n",
      "입양 확률 :  80.5109%\n",
      "입양 확률 :  94.7565%\n",
      "입양 확률 :  78.0636%\n",
      "입양 확률 :  59.0956%\n",
      "입양 확률 :  61.4141%\n",
      "입양 확률 :  25.9249%\n",
      "입양 확률 :  94.6280%\n",
      "입양 확률 :  94.1429%\n",
      "입양 확률 :  59.0332%\n",
      "입양 확률 :  90.5268%\n",
      "입양 확률 :  89.4795%\n",
      "입양 확률 :  90.4354%\n",
      "입양 확률 :  28.0512%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  31.5406%\n",
      "입양 확률 :  35.0414%\n",
      "입양 확률 :  29.4471%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  86.8184%\n",
      "입양 확률 :  44.3399%\n",
      "입양 확률 :  70.0254%\n",
      "입양 확률 :  89.8535%\n",
      "입양 확률 :  43.6976%\n",
      "입양 확률 :  49.5257%\n",
      "입양 확률 :  24.8949%\n",
      "입양 확률 :  31.8996%\n",
      "입양 확률 :  28.3564%\n",
      "입양 확률 :  69.9115%\n",
      "입양 확률 :  87.0251%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  66.9685%\n",
      "입양 확률 :  78.0211%\n",
      "입양 확률 :  39.7200%\n",
      "입양 확률 :  29.8014%\n",
      "입양 확률 :  87.8142%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  92.1527%\n",
      "입양 확률 :  82.1423%\n",
      "입양 확률 :  49.8603%\n",
      "입양 확률 :  91.5583%\n",
      "입양 확률 :  73.2581%\n",
      "입양 확률 :  89.3106%\n",
      "입양 확률 :  86.8845%\n",
      "입양 확률 :  36.3459%\n",
      "입양 확률 :  74.8865%\n",
      "입양 확률 :  26.7738%\n",
      "입양 확률 :  76.2915%\n",
      "입양 확률 :  55.9331%\n",
      "입양 확률 :  91.4863%\n",
      "입양 확률 :  29.6403%\n",
      "입양 확률 :  77.2823%\n",
      "입양 확률 :  84.6162%\n",
      "입양 확률 :  44.8503%\n",
      "입양 확률 :  80.0782%\n",
      "입양 확률 :  32.7578%\n",
      "입양 확률 :  39.8271%\n",
      "입양 확률 :  37.7214%\n",
      "입양 확률 :  41.7302%\n",
      "입양 확률 :  90.8608%\n",
      "입양 확률 :  31.8540%\n",
      "입양 확률 :  43.6976%\n",
      "입양 확률 :  39.6527%\n",
      "입양 확률 :  40.5991%\n",
      "입양 확률 :  86.8982%\n",
      "입양 확률 :  46.6477%\n",
      "입양 확률 :  34.0550%\n",
      "입양 확률 :  91.7288%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  82.7919%\n",
      "입양 확률 :  39.7626%\n",
      "입양 확률 :  75.2077%\n",
      "입양 확률 :  38.4672%\n",
      "입양 확률 :  80.2841%\n",
      "입양 확률 :  74.9565%\n",
      "입양 확률 :  80.4142%\n",
      "입양 확률 :  84.9896%\n",
      "입양 확률 :  72.0752%\n",
      "입양 확률 :  27.7959%\n",
      "입양 확률 :  52.4399%\n",
      "입양 확률 :  54.0417%\n",
      "입양 확률 :  40.3163%\n",
      "입양 확률 :  55.1982%\n",
      "입양 확률 :  95.1264%\n",
      "입양 확률 :  73.9199%\n",
      "입양 확률 :  26.0297%\n",
      "입양 확률 :  49.9497%\n",
      "입양 확률 :  40.4894%\n",
      "입양 확률 :  71.9049%\n",
      "입양 확률 :  80.2524%\n",
      "입양 확률 :  53.8228%\n",
      "입양 확률 :  30.5333%\n",
      "입양 확률 :  36.2937%\n",
      "입양 확률 :  44.1794%\n",
      "입양 확률 :  28.2019%\n",
      "입양 확률 :  71.3230%\n",
      "입양 확률 :  71.3230%\n",
      "입양 확률 :  62.0971%\n",
      "입양 확률 :  67.3646%\n",
      "입양 확률 :  91.4151%\n",
      "입양 확률 :  78.5369%\n",
      "입양 확률 :  33.3205%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  60.2206%\n",
      "입양 확률 :  24.6817%\n",
      "입양 확률 :  93.9052%\n",
      "입양 확률 :  67.0049%\n",
      "입양 확률 :  67.2633%\n",
      "입양 확률 :  48.2329%\n",
      "입양 확률 :  87.2070%\n",
      "입양 확률 :  37.5005%\n",
      "입양 확률 :  55.5821%\n",
      "입양 확률 :  86.9302%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  29.1846%\n",
      "입양 확률 :  83.3939%\n",
      "입양 확률 :  79.4073%\n",
      "입양 확률 :  30.9433%\n",
      "입양 확률 :  32.0811%\n",
      "입양 확률 :  30.2279%\n",
      "입양 확률 :  56.1816%\n",
      "입양 확률 :  29.3114%\n",
      "입양 확률 :  88.4804%\n",
      "입양 확률 :  37.4983%\n",
      "입양 확률 :  85.4370%\n",
      "입양 확률 :  29.4495%\n",
      "입양 확률 :  34.0754%\n",
      "입양 확률 :  91.5203%\n",
      "입양 확률 :  40.3446%\n",
      "입양 확률 :  40.7096%\n",
      "입양 확률 :  82.0429%\n",
      "입양 확률 :  43.7205%\n",
      "입양 확률 :  94.1259%\n",
      "입양 확률 :  93.7749%\n",
      "입양 확률 :  32.7578%\n",
      "입양 확률 :  89.7685%\n",
      "입양 확률 :  57.5454%\n",
      "입양 확률 :  27.6363%\n",
      "입양 확률 :  33.2161%\n",
      "입양 확률 :  23.2291%\n",
      "입양 확률 :  42.7267%\n",
      "입양 확률 :  85.2137%\n",
      "입양 확률 :  40.6669%\n",
      "입양 확률 :  84.5175%\n",
      "입양 확률 :  54.4749%\n",
      "입양 확률 :  48.5200%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  28.2052%\n",
      "입양 확률 :  76.6274%\n",
      "입양 확률 :  90.2871%\n",
      "입양 확률 :  63.5538%\n",
      "입양 확률 :  25.3717%\n",
      "입양 확률 :  46.1768%\n",
      "입양 확률 :  45.3451%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  36.7390%\n",
      "입양 확률 :  36.3324%\n",
      "입양 확률 :  88.9029%\n",
      "입양 확률 :  33.5215%\n",
      "입양 확률 :  83.5877%\n",
      "입양 확률 :  84.4071%\n",
      "입양 확률 :  44.9242%\n",
      "입양 확률 :  93.1353%\n",
      "입양 확률 :  93.0980%\n",
      "입양 확률 :  71.1721%\n",
      "입양 확률 :  33.2620%\n",
      "입양 확률 :  88.0267%\n",
      "입양 확률 :  27.7433%\n",
      "입양 확률 :  82.5484%\n",
      "입양 확률 :  23.8201%\n",
      "입양 확률 :  70.6739%\n",
      "입양 확률 :  27.7433%\n",
      "입양 확률 :  90.6982%\n",
      "입양 확률 :  27.3487%\n",
      "입양 확률 :  34.1240%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  47.1705%\n",
      "입양 확률 :  51.7130%\n",
      "입양 확률 :  33.3122%\n",
      "입양 확률 :  29.6297%\n",
      "입양 확률 :  76.7626%\n",
      "입양 확률 :  53.2499%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  76.6945%\n",
      "입양 확률 :  90.6940%\n",
      "입양 확률 :  88.4009%\n",
      "입양 확률 :  75.0052%\n",
      "입양 확률 :  23.1924%\n",
      "입양 확률 :  37.5005%\n",
      "입양 확률 :  30.2065%\n",
      "입양 확률 :  54.0701%\n",
      "입양 확률 :  68.7554%\n",
      "입양 확률 :  84.6845%\n",
      "입양 확률 :  36.9821%\n",
      "입양 확률 :  78.4158%\n",
      "입양 확률 :  36.5923%\n",
      "입양 확률 :  40.4196%\n",
      "입양 확률 :  86.4652%\n",
      "입양 확률 :  82.5119%\n",
      "입양 확률 :  24.6226%\n",
      "입양 확률 :  30.2970%\n",
      "입양 확률 :  69.4716%\n",
      "입양 확률 :  93.0375%\n",
      "입양 확률 :  33.6043%\n",
      "입양 확률 :  90.0557%\n",
      "입양 확률 :  50.7865%\n",
      "입양 확률 :  33.3205%\n",
      "입양 확률 :  32.4314%\n",
      "입양 확률 :  82.5378%\n",
      "입양 확률 :  29.6516%\n",
      "입양 확률 :  74.6187%\n",
      "입양 확률 :  33.0861%\n",
      "입양 확률 :  76.5208%\n",
      "입양 확률 :  39.1582%\n",
      "입양 확률 :  39.1582%\n",
      "입양 확률 :  27.7513%\n",
      "입양 확률 :  85.2101%\n",
      "입양 확률 :  29.1949%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  34.1780%\n",
      "입양 확률 :  37.3953%\n",
      "입양 확률 :  39.9117%\n",
      "입양 확률 :  30.6148%\n",
      "입양 확률 :  27.3846%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  55.7829%\n",
      "입양 확률 :  84.6056%\n",
      "입양 확률 :  29.5503%\n",
      "입양 확률 :  80.6437%\n",
      "입양 확률 :  48.7735%\n",
      "입양 확률 :  29.0381%\n",
      "입양 확률 :  85.7431%\n",
      "입양 확률 :  29.5508%\n",
      "입양 확률 :  29.5007%\n",
      "입양 확률 :  29.5007%\n",
      "입양 확률 :  36.3402%\n",
      "입양 확률 :  22.9240%\n",
      "입양 확률 :  68.0388%\n",
      "입양 확률 :  45.0437%\n",
      "입양 확률 :  51.8110%\n",
      "입양 확률 :  91.0538%\n",
      "입양 확률 :  92.6616%\n",
      "입양 확률 :  27.0242%\n",
      "입양 확률 :  36.8133%\n",
      "입양 확률 :  73.4615%\n",
      "입양 확률 :  64.4615%\n",
      "입양 확률 :  92.7757%\n",
      "입양 확률 :  82.4978%\n",
      "입양 확률 :  56.0847%\n",
      "입양 확률 :  57.7592%\n",
      "입양 확률 :  73.3543%\n",
      "입양 확률 :  43.9328%\n",
      "입양 확률 :  84.2009%\n",
      "입양 확률 :  43.5355%\n",
      "입양 확률 :  22.8924%\n",
      "입양 확률 :  72.6577%\n",
      "입양 확률 :  29.1657%\n",
      "입양 확률 :  88.6260%\n",
      "입양 확률 :  42.2912%\n",
      "입양 확률 :  36.0405%\n",
      "입양 확률 :  94.3621%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.0398%\n",
      "입양 확률 :  61.8593%\n",
      "입양 확률 :  29.5317%\n",
      "입양 확률 :  75.3754%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  54.0260%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  55.1988%\n",
      "입양 확률 :  42.3845%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  42.0999%\n",
      "입양 확률 :  58.0729%\n",
      "입양 확률 :  31.4567%\n",
      "입양 확률 :  25.0710%\n",
      "입양 확률 :  28.8452%\n",
      "입양 확률 :  94.4262%\n",
      "입양 확률 :  38.4951%\n",
      "입양 확률 :  36.7675%\n",
      "입양 확률 :  20.2064%\n",
      "입양 확률 :  28.9213%\n",
      "입양 확률 :  77.4118%\n",
      "입양 확률 :  44.3877%\n",
      "입양 확률 :  91.2746%\n",
      "입양 확률 :  58.3606%\n",
      "입양 확률 :  58.3606%\n",
      "입양 확률 :  57.1406%\n",
      "입양 확률 :  94.4116%\n",
      "입양 확률 :  66.8996%\n",
      "입양 확률 :  36.2556%\n",
      "입양 확률 :  89.6569%\n",
      "입양 확률 :  77.3612%\n",
      "입양 확률 :  79.5870%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  91.3021%\n",
      "입양 확률 :  21.5133%\n",
      "입양 확률 :  21.5133%\n",
      "입양 확률 :  53.4980%\n",
      "입양 확률 :  39.2845%\n",
      "입양 확률 :  42.2721%\n",
      "입양 확률 :  32.6121%\n",
      "입양 확률 :  33.0232%\n",
      "입양 확률 :  91.6300%\n",
      "입양 확률 :  87.2243%\n",
      "입양 확률 :  86.9037%\n",
      "입양 확률 :  20.0483%\n",
      "입양 확률 :  50.5222%\n",
      "입양 확률 :  44.1166%\n",
      "입양 확률 :  57.5966%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  85.0742%\n",
      "입양 확률 :  92.5027%\n",
      "입양 확률 :  33.8111%\n",
      "입양 확률 :  35.8180%\n",
      "입양 확률 :  73.8789%\n",
      "입양 확률 :  19.6655%\n",
      "입양 확률 :  20.9785%\n",
      "입양 확률 :  48.2864%\n",
      "입양 확률 :  55.5369%\n",
      "입양 확률 :  82.0286%\n",
      "입양 확률 :  28.0213%\n",
      "입양 확률 :  25.0031%\n",
      "입양 확률 :  71.9830%\n",
      "입양 확률 :  69.3335%\n",
      "입양 확률 :  94.2947%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  79.8950%\n",
      "입양 확률 :  79.5124%\n",
      "입양 확률 :  65.0101%\n",
      "입양 확률 :  92.5092%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입양 확률 :  28.6237%\n",
      "입양 확률 :  93.3353%\n",
      "입양 확률 :  32.6441%\n",
      "입양 확률 :  37.7122%\n",
      "입양 확률 :  67.2223%\n",
      "입양 확률 :  23.0380%\n",
      "입양 확률 :  70.3892%\n",
      "입양 확률 :  77.7126%\n",
      "입양 확률 :  55.1501%\n",
      "입양 확률 :  57.7917%\n",
      "입양 확률 :  67.0752%\n",
      "입양 확률 :  32.6870%\n",
      "입양 확률 :  88.6794%\n",
      "입양 확률 :  50.1714%\n",
      "입양 확률 :  34.4334%\n",
      "입양 확률 :  30.2486%\n",
      "입양 확률 :  57.1593%\n",
      "입양 확률 :  53.8719%\n",
      "입양 확률 :  71.0483%\n",
      "입양 확률 :  66.8481%\n",
      "입양 확률 :  83.2895%\n",
      "입양 확률 :  59.9851%\n",
      "입양 확률 :  87.3223%\n",
      "입양 확률 :  94.8906%\n",
      "입양 확률 :  53.2306%\n",
      "입양 확률 :  41.5467%\n",
      "입양 확률 :  41.7502%\n",
      "입양 확률 :  26.5065%\n",
      "입양 확률 :  29.7671%\n",
      "입양 확률 :  96.1752%\n",
      "입양 확률 :  44.4502%\n",
      "입양 확률 :  92.8657%\n",
      "입양 확률 :  39.4909%\n",
      "입양 확률 :  83.0424%\n",
      "입양 확률 :  91.3181%\n",
      "입양 확률 :  86.7357%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  23.3083%\n",
      "입양 확률 :  24.4934%\n",
      "입양 확률 :  90.5356%\n",
      "입양 확률 :  65.2899%\n",
      "입양 확률 :  32.4555%\n",
      "입양 확률 :  20.8052%\n",
      "입양 확률 :  31.4452%\n",
      "입양 확률 :  50.1362%\n",
      "입양 확률 :  52.8710%\n",
      "입양 확률 :  53.2796%\n",
      "입양 확률 :  53.4489%\n",
      "입양 확률 :  85.6347%\n",
      "입양 확률 :  89.7685%\n",
      "입양 확률 :  30.2853%\n",
      "입양 확률 :  57.8625%\n",
      "입양 확률 :  33.0917%\n",
      "입양 확률 :  52.3523%\n",
      "입양 확률 :  90.5433%\n",
      "입양 확률 :  94.5191%\n",
      "입양 확률 :  36.3402%\n",
      "입양 확률 :  26.6885%\n",
      "입양 확률 :  58.0436%\n",
      "입양 확률 :  34.7650%\n",
      "입양 확률 :  37.8441%\n",
      "입양 확률 :  36.5673%\n",
      "입양 확률 :  52.7714%\n",
      "입양 확률 :  43.0492%\n",
      "입양 확률 :  90.9383%\n",
      "입양 확률 :  31.3785%\n",
      "입양 확률 :  92.8120%\n",
      "입양 확률 :  80.4843%\n",
      "입양 확률 :  56.6195%\n",
      "입양 확률 :  75.4732%\n",
      "입양 확률 :  73.3082%\n",
      "입양 확률 :  69.7706%\n",
      "입양 확률 :  46.3540%\n",
      "입양 확률 :  27.0691%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  32.0362%\n",
      "입양 확률 :  54.5583%\n",
      "입양 확률 :  51.0287%\n",
      "입양 확률 :  56.4868%\n",
      "입양 확률 :  55.1727%\n",
      "입양 확률 :  27.1687%\n",
      "입양 확률 :  79.5345%\n",
      "입양 확률 :  78.0818%\n",
      "입양 확률 :  26.6011%\n",
      "입양 확률 :  27.5421%\n",
      "입양 확률 :  27.5421%\n",
      "입양 확률 :  26.6011%\n",
      "입양 확률 :  35.2598%\n",
      "입양 확률 :  28.5249%\n",
      "입양 확률 :  24.5133%\n",
      "입양 확률 :  82.6250%\n",
      "입양 확률 :  69.2903%\n",
      "입양 확률 :  26.1384%\n",
      "입양 확률 :  48.7082%\n",
      "입양 확률 :  54.5117%\n",
      "입양 확률 :  32.1908%\n",
      "입양 확률 :  70.5507%\n",
      "입양 확률 :  35.4440%\n",
      "입양 확률 :  92.4061%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  31.9113%\n",
      "입양 확률 :  30.7316%\n",
      "입양 확률 :  40.7673%\n",
      "입양 확률 :  32.9232%\n",
      "입양 확률 :  29.0579%\n",
      "입양 확률 :  32.0362%\n",
      "입양 확률 :  35.4440%\n",
      "입양 확률 :  86.6981%\n",
      "입양 확률 :  78.8287%\n",
      "입양 확률 :  35.2400%\n",
      "입양 확률 :  23.0991%\n",
      "입양 확률 :  81.9916%\n",
      "입양 확률 :  39.6089%\n",
      "입양 확률 :  74.2966%\n",
      "입양 확률 :  35.8483%\n",
      "입양 확률 :  77.1089%\n",
      "입양 확률 :  38.0417%\n",
      "입양 확률 :  77.1674%\n",
      "입양 확률 :  75.7726%\n",
      "입양 확률 :  87.7377%\n",
      "입양 확률 :  80.3187%\n",
      "입양 확률 :  64.9833%\n",
      "입양 확률 :  59.9470%\n",
      "입양 확률 :  59.2415%\n",
      "입양 확률 :  83.3474%\n",
      "입양 확률 :  29.4303%\n",
      "입양 확률 :  90.9939%\n",
      "입양 확률 :  81.1342%\n",
      "입양 확률 :  61.0305%\n",
      "입양 확률 :  82.8998%\n",
      "입양 확률 :  76.6702%\n",
      "입양 확률 :  34.4896%\n",
      "입양 확률 :  43.6037%\n",
      "입양 확률 :  32.9014%\n",
      "입양 확률 :  23.4589%\n",
      "입양 확률 :  39.5481%\n",
      "입양 확률 :  49.2593%\n",
      "입양 확률 :  51.5592%\n",
      "입양 확률 :  51.5592%\n",
      "입양 확률 :  88.9153%\n",
      "입양 확률 :  90.1937%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    new_x=train_data[i, :].reshape(1,186)\n",
    "    print('입양 확률 : %8.4f%%' % (l2_model.predict(new_x)*100))\n",
    "l2_model.save('l2_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
